{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: blue;\">Analysing method related differences script - bioinformatic differences</span>\n",
    "\n",
    "This script goes through the investigating discrepancies between the 4 methods\n",
    "\n",
    "1. ABRicate (our baseline method)\n",
    "2. ARIBA\n",
    "3. KmerResistance\n",
    "4. SRST2\n",
    "\n",
    "**NOTE FOR EASE OF READING, SOME CODE IS REPEATED IN EACH SECTION, TO MAKE EACH SECTION BE A DISCREET UNIT OF ANALYSIS, THIS LENGTHANS THE RUNTIME OF THE CODE BUT MAKES IT EASIER TO UNDERSTAND. THE RUNTIME IS STILL SHORT (< 2 hours even comparing all database similarities)**\n",
    "\n",
    "### <span style=\"color: green;\">Highlighting database dependency.</span>\n",
    "\n",
    "For each of several databases, it does the following:\n",
    "1. reads in all the results we obtained\n",
    "3. This is then used to produce **Figure X** \n",
    "\n",
    "\n",
    "Databases used\n",
    "**Primary**\n",
    "Resfinder 1st October 2019 release \n",
    "**Secondary**\n",
    "* CARD 23rd October 2019 release\n",
    "* beta-lactam.fsa of the Resfinder 1st October 2019 release\n",
    "* aminoglycoside.fsa  of the Resfinder 1st October 2019 release\n",
    "* quinolone.fsa of the Resfinder 1st October 2019 release\n",
    "* trimethoprim.fsa of the Resfinder 1st October 2019 release\n",
    "* sulphonamide.fsa of the Resfinder 1st October 2019 release\n",
    "* Resfinder 22nd of January 2019 release\n",
    "* Resfinder 22nd of January 2018 release\n",
    "* Resfinder 26th of January 2017 release\n",
    "\n",
    "\n",
    "\n",
    "From here on, we are investigating causes of error and only use the Resfinder 1st October 2019 release.\n",
    "\n",
    "### <span style=\"color: green;\">Identifying which TRGs are most commonly discrepant and then </span>\n",
    "\n",
    "1. For this we first identify which reported TRGs likely represent the same gene found differently by the different programs (see image below)\n",
    "\n",
    "![image](method_1.png)\n",
    "\n",
    "2. We then look at patterns of discrepancy and identify the 10 most common across the data\n",
    "3. We then demonstrate what we believe the cause of each of these is (simulated example in each cell of the notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: green;\">Highlighting extent of annotation errors</span>\n",
    "\n",
    "This then goes through how we determine if an error is likely due to annotation or not\n",
    "\n",
    "1. For each discrepant, we identify if any of the identified TRGs are complete within the assembly.\n",
    "2. If so, we take the containing contig and simulate **perfect* reads at 50x and 500x from it (10 times each).\n",
    "3. We then observe if the same misclassification pattern occurs again. If so the error is deemed likely annotation\n",
    "4. This is then used to produce **Figure XX** in the paper.\n",
    "\n",
    "Note for some of the code from this section is only presented here in markdown as I can't provide any full sequence files on GitHub. However, the sequences for this project are available on NCBI and the code can be recreated to analyse these scripts here.\n",
    "\n",
    "### <span style=\"color: green;\">Investigating non annotation errors.</span>\n",
    "\n",
    "This section finally ends on investigation of 6 samples where different beta-lcatamase genes were reported but were not artefactual.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Setup</span>\n",
    "\n",
    "**So first steps are to load in required modules and then identify all the output reports**\n",
    "\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "1. Python 3 \n",
    "2. Biopython\n",
    "3. Pandas\n",
    "4. Numpy\n",
    "5. tqdm\n",
    "6. networkx\n",
    "\n",
    "#### Inputs\n",
    "Some notes for this step, firstly which files we take\n",
    "1. for ABRicate we take each contigs.tab file\n",
    "2. for ARIBA we use its summary file\n",
    "3. for KmerResistance we use the .KmerRes files\n",
    "4. for SRST2 we use the .out__fullgenes__seqs_clustered__results.txt files\n",
    "\n",
    "Note these were chosen as they seem to follow guidelines and where no guidelines available, give us the most closely matching results between the four programs\n",
    "\n",
    "Note also SRST2 does not produce output if it finds no genes. The others all do\n",
    "\n",
    "#### Resfinder database\n",
    "\n",
    "For this we load\n",
    "1. The naming link database\n",
    "2. For each of the sub databases by antibiotic class \n",
    "  * beta-lactam\n",
    "  * quinolone\n",
    "  * aminoglycoside\n",
    "  * sulfa antibiotics\n",
    "  * trimethoprim\n",
    "3. The whole database\n",
    "\n",
    "We do 1 so the results are interpretable\n",
    "We do 2 so we can breakdown results by antibiotic class as well as specific genes\n",
    "We do 3 to do all versus all of the resistance database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by_trg_pattern.csv            method_related.ipynb\r\n",
      "interpreting_simulations.csv  notes_random.txt\r\n",
      "isolate_patterns.csv          pattern_annotator.csv\r\n",
      "method_1.png                  readymade_sim_matrix.csv\r\n",
      "method_1.svg                  \u001b[1m\u001b[36mresult_tarballs\u001b[m\u001b[m/\r\n",
      "method_combinations.png       simulation_analysis.ipynb\r\n",
      "method_image.png              simulation_contigs.csv\r\n"
     ]
    }
   ],
   "source": [
    "# We start by importing required modules\n",
    "# File structure\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# These are the fundamental modules used for analysing the data\n",
    "\n",
    "\n",
    "# Pictures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sequence manipulation\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "# These modules are priarily used for loading the database, and then for identifying whether there are any\n",
    "# Perfrect protein matches in the dataset\n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "\n",
    "# Other general\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "# We just import this to check our code runs sensibly and to get timing estimates for stuff\n",
    "from copy import deepcopy\n",
    "\n",
    "#Looking at local files\n",
    "%ls \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-5724581c4f27>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-5724581c4f27>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    \"fasta\":\"../db_preparation/resfinder_20191001_blm/blm_formatted.fasta\",\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# So for the first bit of the analysis were going to look at the results from different databases. \n",
    "\n",
    "\n",
    "# Following dictionary links tarballs (for results), and database files, so that all can be analysed together\n",
    "# All tarballs available in XXX. please run tar -xzvf * on all tarballs prior to running this code\n",
    "# Note we don't provide all the results as the files become large and difficult to work with in this small repo\n",
    "# Instead we just keep the files we need. These can all be reproduced by accding the sequence files directly and \n",
    "# Then reruning each piece of software using the databases provided.\n",
    "\n",
    "if os.name == \"posix\":\n",
    "    dbresult_link = {\"resfiner_20191001_full\":{\"results\":\"result_tarballs/resfinderfull_20191001/\",\n",
    "                                              \"fasta\": \"../db_preparation/resfinder_20191001/resfinder_20191001_formatted.fasta\", \n",
    "                                              \"linkfile\": \"../db_preparation/resfinder_20191001/resfinder_20191001_link.csv\"},\n",
    "                     \"resfinder_20191001_blm\":{\"results\":\"result_tarballs/resfinderblm_20191001/\",\n",
    "                                              \"fasta\":\"../db_preparation/resfinder_20191001_blm/blm_formatted.fasta\",\n",
    "                                              \"linkfile\":\"../db_preparation/resfinder_20191001_blm/resfinder_blm_link.csv\"},\n",
    "                     \"resfinder_20191001_ami\":{\"results\":\"result_tarballs/resfinderami_20191001/\",\n",
    "                                              \"fasta\":\"../db_preparation/resfinder_20191001_ami/ami_formatted.fasta\",\n",
    "                                              \"linkfile\":\"../db_preparation/resfinder_20191001_ami/resfinder_ami_link.csv\"},\n",
    "                     \"resfinder_20191001_qui\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                               \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20191001_tri\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20191001_sul\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20190122_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20180122_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20170126_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"card_20191023_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                           \"fasta\":\"\",\n",
    "                                           \"linkfile\":\"\"}\n",
    "                }\n",
    "elif os.name == 'nt':\n",
    "    dbresult_link = {\"resfiner_20191001_full\":{\"abricate_results\":\"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/collated_output/\",\n",
    "                                               \"ariba_results\":\"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/collated_output/\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\": \"../resistance_databases/resfinder_20191001_formatted.fasta\", \n",
    "                                              \"linkfile\": \"\"},\n",
    "                     \"resfinder_20191001_blm\":{\"abricate_results\":\"\", \n",
    "                                               \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\", \n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20191001_ami\":{\"abricate_results\":\"\", \n",
    "                                               \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\", \n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20191001_qui\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                               \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20191001_tri\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20191001_sul\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20190122_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20180122_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"resfinder_20170126_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                              \"fasta\":\"\",\n",
    "                                              \"linkfile\":\"\"},\n",
    "                     \"card_20191023_full\":{\"abricate_results\":\"\", \n",
    "                                              \"ariba_results\":\"\",\n",
    "                                               \"kmerres_results\":\"\",\n",
    "                                               \"srst2_results\":\"\",\n",
    "                                           \"fasta\":\"\",\n",
    "                                           \"linkfile\":\"\"}\n",
    "                    }\n",
    "\n",
    "# Loading in the output files\n",
    "\n",
    "abricate_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/collated_output/abricate_summary/\"\n",
    "ariba_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/collated_output/ariba_summary/\"\n",
    "kmerres_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/collated_output/kmerres_summary/\"\n",
    "srst2_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/collated_output/srst2_summary/\"\n",
    "\n",
    "ariba_summary = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/collated_output/ariba_summary.csv\"\n",
    "\n",
    "# ABRicate\n",
    "abricate_files = [os.path.join(root, f) for root, dirs, files \n",
    "                  in os.walk(abricate_dir)\n",
    "                 for f in files if f != \"summary.tab\"]\n",
    "guuids = [k.split(\"/\")[-1].split(\"_\")[0] for k in abricate_files]\n",
    "abricate_files = {k.split(\"/\")[-1].split(\"_\")[0]:k for k in abricate_files}\n",
    "\n",
    "#KmerResistance\n",
    "kmerres_files = [os.path.join(root, f) for root, dirs, files in os.walk(kmerres_dir)\n",
    "                 for f in files if \".KmerRes\" in f]\n",
    "kmerres_files = {k.split(\"/\")[-1].split(\"_\")[0]:k for k in kmerres_files}\n",
    "\n",
    "#SRST2\n",
    "srst2_files = [os.path.join(root, f) for root, dirs, files in os.walk(srst2_dir) for f in files]\n",
    "srst2_files = {k.split(\"/\")[-1].split(\"_\")[0]:k for k in srst2_files}\n",
    "\n",
    "#ARIBA\n",
    "ariba_names = [k.split(\"/\")[1].split(\"_\")[0] for k in list(pd.read_csv(ariba_summary).name)]\n",
    "ariba_summary = pd.read_csv(ariba_summary).fillna(\"\")\n",
    "ariba_summary.index = ariba_names\n",
    "\n",
    "\n",
    "# Note because of the different way in which ARIBA is loaded in we also add a special reading function\n",
    "def ariba_parser(s):\n",
    "    s_clusters = sorted(list(set([k.split(\".\")[0] for k in s.index if \"cluster\" in k and \".match\" in k])))\n",
    "    s_clusters = [k for k in s_clusters if s[k+\".match\"] == \"yes\"]\n",
    "    s_genes = [s[k+ \".ref_seq\"] for k in s_clusters]\n",
    "    return s_genes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../resistance_databases/resfinder_20191001_link.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a3093876e9c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 1. loading the link file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresfinder_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mresfinder_rlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../resistance_databases/resfinder_20191001_link.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Finally loading in the resfinder database\n",
    "linkfile = \"../resistance_databases/resfinder_20191001_link.csv\"\n",
    "resfinder_fulldb = \"../resistance_databases/resfinder_20191001_formatted.fasta\"\n",
    "blm_db = \"../resistance_databases/resfinder_20191001/beta-lactam.fsa\"\n",
    "qui_db = \"../resistance_databases/resfinder_20191001/quinolone.fsa\"\n",
    "ami_db = \"../resistance_databases/resfinder_20191001/aminoglycoside.fsa\"\n",
    "sul_db = \"../resistance_databases/resfinder_20191001/sulphonamide.fsa\"\n",
    "tri_db = \"../resistance_databases/resfinder_20191001/trimethoprim.fsa\"\n",
    "\n",
    "# 1. loading the link file\n",
    "resfinder_link = pd.read_csv(linkfile, index_col=0, header=None)\n",
    "resfinder_rlink = pd.read_csv(linkfile, index_col=1, header=None)\n",
    "\n",
    "# 2. loading the sub files\n",
    "# Note this removes duplicate Ids, this was checked as to what it did in the database formatting files\n",
    "blm_db = {k.id:k for k in SeqIO.parse(blm_db, \"fasta\")}\n",
    "qui_db = {k.id:k for k in SeqIO.parse(qui_db, \"fasta\")}\n",
    "ami_db = {k.id:k for k in SeqIO.parse(ami_db, \"fasta\")}\n",
    "sul_db = {k.id:k for k in SeqIO.parse(sul_db, \"fasta\")}\n",
    "tri_db = {k.id:k for k in SeqIO.parse(tri_db, \"fasta\")}\n",
    "\n",
    "# 3. loading the whole database\n",
    "res_db = SeqIO.to_dict(SeqIO.parse(resfinder_fulldb, \"fasta\"))\n",
    "\n",
    "# Next were going to do all vs all similarity of the resfinder database\n",
    "\n",
    "sim_matrix = pd.DataFrame(np.zeros((len(res_db.keys()),len(res_db.keys()) )), \n",
    "                         columns = sorted(list(res_db.keys())), index=sorted(list(res_db.keys())))\n",
    "# For this similarity we will use 17-mers (one of the prefixes using MASH, note several others were checked prior to this for their effects)\n",
    "# Clusters marked by this are fairly similar to other k-mer sizes\n",
    "res_db_kmers = {k: set([str(res_db[k].seq)[i:i+17] for i in range(len(res_db[k].seq)-16)]) for k in res_db.keys()}\n",
    "\n",
    "def calculate_jac_sim(l1, l2):\n",
    "    intersection = len(res_db_kmers[l1].intersection(res_db_kmers[l2]))\n",
    "    union = len(res_db_kmers[l1].union(res_db_kmers[l2]))\n",
    "    return(intersection/union)\n",
    "\n",
    "# This code actually goes through populating this matrix, \n",
    "# However takes 30 minutes to run, and for the sake of running this code quickly on line, I will\n",
    "# use a matrix I made earlier (using thie code)\n",
    "# for n in tnrange(len(res_db_kmers)):\n",
    "#     k  = list(res_db_kmers.keys())[n]\n",
    "#     for j in res_db_kmers:\n",
    "#         sim_matrix.loc[k][j] = calculate_jac_sim(k, j)\n",
    "\n",
    "sim_matrix = pd.read_csv(\"readymade_sim_matrix.csv\", index_col = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the analysis class\n",
    "\n",
    "\n",
    "In the next section of code, the aim is to define a class which performs most of the comparisons for us. \n",
    "\n",
    "#### Defining useful functions\n",
    "Before we set up the class we define useful functions\n",
    "**CLUSTERING**\n",
    "recursive cluster => This essentially uses netrowkx to greate a graph, which links togehter elements with non-zero similarity. \n",
    "The other two functions make_tuples and name list are simplications of bits within the recursive cluster functions\n",
    "\n",
    "**AGREEMENT**\n",
    "\n",
    "![image](method_combinations.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Reading in the data\n",
    "\n",
    "This does the following steps\n",
    "For each of ABRicate, ARIBA, KmerResistance, SRST2 we\n",
    "1. read in its file\n",
    "2. Pull out the TRGs it identifies and relabel them with their original names\n",
    "3. Separate these into groups according to their relavent antibiotics.\n",
    "\n",
    "\n",
    "\n",
    "#### Comparing outputs\n",
    "\n",
    "The next stage is to define what genes we have in the sample, here we then first\n",
    "1. make a combined dict of all genes found. \n",
    "2. Then separate according to clusters\n",
    "3. make identification patterns for each of these clusters\n",
    "\n",
    "\n",
    "\n",
    "This is done using an external spreadsheet (which suggests putative families for all patterns of genes seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define three useful functions we use later \n",
    "\n",
    "\n",
    "###### CLUSTERING FUNCTIONS  ######\n",
    "\n",
    "\n",
    "def make_tuples(l):\n",
    "    # Make all possible tuples in a list\n",
    "    output = []\n",
    "    for i in range(len(l)):\n",
    "        for j in range(len(l)):\n",
    "            output.append((l[i],l[j]))\n",
    "    output = sorted(list(set(output)))\n",
    "    return output\n",
    "\n",
    "def name_list(l, d):\n",
    "    #rename all elements of a list\n",
    "    return [d[k] for k in l]\n",
    "\n",
    "\n",
    "def recursive_cluster(df, l):\n",
    "    groups = {}\n",
    "    # First we get all linked pais.\n",
    "    for i in l:\n",
    "        i_data = df.loc[i]\n",
    "        i_group = [i]\n",
    "        for j in l:\n",
    "            if j != i: \n",
    "                if i_data[j] != 0:\n",
    "                    i_group.append(j)\n",
    "        i_group = sorted(i_group)\n",
    "        groups[i] =  i_group\n",
    "    # Assign numbers to the elements of l and then generate a dictionary to link numbers and names\n",
    "    naming = {}\n",
    "    reverse_naming  = {}\n",
    "    m = 1\n",
    "    for i in l:\n",
    "        naming[i] = m\n",
    "        reverse_naming[m] = i\n",
    "        m += 1\n",
    "    # Grouping tuples like a graph using networkx\n",
    "    final_tuples = []\n",
    "    for i in groups:\n",
    "        final_tuples = final_tuples + make_tuples([naming[j] for j in groups[i]])\n",
    "    final_tuples = sorted(list(set(final_tuples)))\n",
    "    graph=nx.Graph(final_tuples)\n",
    "    output = [name_list(list(c), reverse_naming) for c in nx.connected_components(graph)]\n",
    "    return output\n",
    "\n",
    "###### AGREEMENT PATTERN FUNCTIONS \n",
    "\n",
    "# Note for these functions they always assume the results are put in the correct order\n",
    "# i.e. ABRicate, ARIBA, KmerResistance, SRST2\n",
    "\n",
    "# First we start with a general agreement function \n",
    "def agreement_pattern(l1, l2, l3, l4):\n",
    "    args = deepcopy(locals())\n",
    "    arg_list = ['l1', 'l2', 'l3', 'l4']\n",
    "    for key in arg_list:\n",
    "        args[key] = \":\".join(sorted(args[key]))\n",
    "    patterns = {}\n",
    "    output = []\n",
    "    starting_no = 0\n",
    "    for key in arg_list:\n",
    "        if args[key] not in patterns:\n",
    "            starting_no += 1\n",
    "            patterns[args[key]] = starting_no\n",
    "            output.append(starting_no)\n",
    "        else:\n",
    "            output.append(patterns[args[key]])\n",
    "    return output\n",
    "    \n",
    "\n",
    "# Now for gene agreement, I use this program to say which genes (from a list) each method has found\n",
    "def pres_bin(l1, l2):\n",
    "    output = []\n",
    "    for k in l1:\n",
    "        if k in l2:\n",
    "            output.append(\"1\")\n",
    "        else:\n",
    "            output.append(\"0\")\n",
    "    return output\n",
    "\n",
    "# Here is the agreement function again, but this time i've dropped the sort function. \n",
    "# This enables me to use the output from pres_bin directly to make the patterns as defined above \n",
    "def pres_bin_agreement_pattern(l1, l2, l3, l4):\n",
    "    args = deepcopy(locals())\n",
    "    arg_list = ['l1', 'l2', 'l3', 'l4']\n",
    "    for key in arg_list:\n",
    "        args[key] = \":\".join(args[key])\n",
    "    patterns = {}\n",
    "    output = []\n",
    "    starting_no = 0\n",
    "    for key in arg_list:\n",
    "        if args[key] not in patterns:\n",
    "            starting_no += 1\n",
    "            patterns[args[key]] = starting_no\n",
    "            output.append(starting_no)\n",
    "        else:\n",
    "            output.append(patterns[args[key]])\n",
    "    return output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading in each of the data\n",
    "\n",
    "class isolate:\n",
    "    \n",
    "    def __init__(self,guuid):\n",
    "        self.guuid = guuid\n",
    "        # First lest start with ABRicate, for this one we take everything assuming control\n",
    "        # Each section of this code does similar things, 1. read the file , 2 translate the genes, \n",
    "        # 3 separate by the classes were interested in.\n",
    "        self.abricate_fl = pd.read_csv(abricate_files[self.guuid], delimiter= \"\\t\").fillna(\"\")\n",
    "        self.abricate_fl = self.abricate_fl.loc[self.abricate_fl['%COVERAGE'] > 60.0]\n",
    "        self.abricate_fl = self.abricate_fl.loc[self.abricate_fl['%IDENTITY'] > 90.0]\n",
    "        self.abricate_genes = sorted(list(set([resfinder_link.loc[k][1] for k in list(self.abricate_fl['GENE'])])))\n",
    "        self.abricate_trg = {\"blm\":sorted([k for k in self.abricate_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.abricate_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.abricate_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.abricate_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.abricate_genes if k in tri_db.keys()])}\n",
    "        self.abricate_alltrg = sorted(list(set(self.abricate_trg['blm'] + self.abricate_trg['qui'] + \n",
    "                                               self.abricate_trg['ami'] + self.abricate_trg['sul'] +\n",
    "                                              self.abricate_trg['tri'])))\n",
    "        \n",
    "        # Next ARIBA\n",
    "        self.ariba_data = ariba_parser(ariba_summary.loc[self.guuid])\n",
    "        self.ariba_genes = sorted(list(set([resfinder_link.loc[k][1] for k in self.ariba_data])))\n",
    "        self.ariba_trg = {\"blm\":sorted([k for k in self.ariba_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.ariba_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.ariba_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.ariba_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.ariba_genes if k in tri_db.keys()])}\n",
    "        self.ariba_alltrg = sorted(list(set(self.ariba_trg['blm'] + self.ariba_trg['qui'] + \n",
    "                                           self.ariba_trg['ami'] + self.ariba_trg['sul'] + \n",
    "                                            self.ariba_trg['tri'])))\n",
    "        \n",
    "        # Next KmerResistance\n",
    "        # For this one we also add in a coverage cut-off given our file doesn't seem to be able to do this reliably\n",
    "        # Plus we are trying to apply the 70% cutoff as it doesn't work easily in the coverage\n",
    "        # So we will re-apply this.)\n",
    "        self.kmerres_fl = pd.read_csv(kmerres_files[self.guuid], delimiter = \"\\t\").fillna(\"\")\n",
    "        self.kmerres_fl = self.kmerres_fl.loc[self.kmerres_fl.template_id > 70.0]\n",
    "        self.kmerres_genes = sorted(list(set([resfinder_link.loc[k][1] for k in [j for j in list(self.kmerres_fl['#Template']) if \"resfindernewid\" in j]])))\n",
    "        self.kmerres_trg = {\"blm\":sorted([k for k in self.kmerres_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.kmerres_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.kmerres_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.kmerres_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.kmerres_genes if k in tri_db.keys()])}\n",
    "        self.kmerres_alltrg = sorted(list(set(self.kmerres_trg['blm'] + self.kmerres_trg['qui'] + self.kmerres_trg['ami'] + self.kmerres_trg['sul'] + self.kmerres_trg['tri'] )))\n",
    "        \n",
    "        # Note for SRST2 we have another bit which doesn't quite work\n",
    "        # It does not make a file if it finds no genes\n",
    "        # Therefore we put it into a try except group\n",
    "\n",
    "        try:\n",
    "            self.srst2_fl = pd.read_csv(srst2_files[self.guuid], delimiter = \"\\t\").fillna(\"\")\n",
    "            self.srst2_genes = sorted(list(set([resfinder_link.loc[k][1] for k in list(self.srst2_fl['allele'])])))\n",
    "        except:\n",
    "            self.srst2_fl = \"N/A\"\n",
    "            self.srst2_genes = []\n",
    "        self.srst2_trg = {\"blm\":sorted([k for k in self.srst2_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.srst2_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.srst2_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.srst2_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.srst2_genes if k in tri_db.keys()])} \n",
    "        self.srst2_alltrg = sorted(list(set(self.srst2_trg['blm'] + self.srst2_trg['qui'] + \n",
    "                                           self.srst2_trg['ami'] + self.srst2_trg['sul'] + \n",
    "                                            self.srst2_trg['tri'])))\n",
    "        \n",
    "        \n",
    "        ### Aggregating genes. \n",
    "        \n",
    "        self.all_trg = {}\n",
    "        for k in [\"blm\", 'qui', 'ami', 'sul', 'tri']:\n",
    "            self.all_trg[k] = sorted(list(set(self.abricate_trg[k]).union(self.ariba_trg[k]).union(self.kmerres_trg[k]).union(self.srst2_trg[k])))\n",
    "        self.geno_full = {\"abricate\":self.abricate_alltrg, \"ariba\":self.ariba_alltrg, \n",
    "                         \"kmerres\": self.kmerres_alltrg, \"srst2\":self.srst2_alltrg}\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Defining gene families \n",
    "        self.gene_families = {}\n",
    "        for k in self.all_trg.keys():\n",
    "            k_df = pd.DataFrame(np.zeros((len(self.all_trg[k]),len(self.all_trg[k]) )), \n",
    "                               columns = self.all_trg[k], index=self.all_trg[k])\n",
    "            for l in k_df.index:\n",
    "                for j in k_df.columns:\n",
    "                    k_df.loc[l][j] = sim_matrix.loc[resfinder_rlink.loc[l][0]][resfinder_rlink.loc[j][0]]\n",
    "            self.gene_families[k] = recursive_cluster(k_df, k_df.index)\n",
    "            \n",
    "            \n",
    "        ### Assessing levels of agreement\n",
    "        # Here we define three things, Firstly, do results agree for all genes for a particular antibiotic class\n",
    "        # Then do they agree for a whole isolate\n",
    "        # Then finally we do a bit more delving into the patterns of disagreement\n",
    "        # Whole isolate level agreement\n",
    "        \n",
    "        # Firstly for eac abx class\n",
    "        self.abx_patterns = {}\n",
    "        for key in [\"blm\", 'qui', 'ami', 'sul', 'tri']:\n",
    "            self.abx_patterns[key] = agreement_pattern(self.abricate_trg[key], \n",
    "                                                      self.ariba_trg[key], \n",
    "                                                      self.kmerres_trg[key], \n",
    "                                                      self.srst2_trg[key])\n",
    "        self.abx_agreement = {}\n",
    "        for key in [\"blm\", 'qui', 'ami', 'sul', 'tri']:\n",
    "            self.abx_agreement[key] = (self.abx_patterns[key] == [1, 1, 1, 1])\n",
    "\n",
    "        \n",
    "        # Then for each whole isolate\n",
    "        self.full_agreement = (list(set(list(self.abx_agreement.values()))) == [True])\n",
    "        self.full_patterns = agreement_pattern(self.abricate_alltrg, self.ariba_alltrg, \n",
    "                                               self.kmerres_alltrg, self.srst2_alltrg)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Note there is a difference between full =  all the patterns in the 5 gene families I have\n",
    "        # and full across the entiritty of resfinder , this attribute reflects this\n",
    "        self.wholeresfinder_patterns = agreement_pattern(sorted(self.abricate_genes), sorted(self.ariba_genes),\n",
    "                                               sorted(self.kmerres_genes), sorted(self.srst2_genes))\n",
    "        self.wholeresfinder_agreement = (self.wholeresfinder_patterns == [1,1,1,1])\n",
    "\n",
    "\n",
    "        \n",
    "        # Lastly for each gene\n",
    "        self.genes_identified = {}\n",
    "        self.gene_patterns = {}\n",
    "        self.gene_group = {}\n",
    "        for abx in self.gene_families:\n",
    "            for pat in self.gene_families[abx]:\n",
    "                pat_id = \":\".join(pat)\n",
    "                pat_string = [pres_bin(pat, self.abricate_alltrg),\n",
    "                              pres_bin(pat, self.ariba_alltrg), \n",
    "                              pres_bin(pat, self.kmerres_alltrg), \n",
    "                              pres_bin(pat, self.srst2_alltrg)]\n",
    "                self.gene_patterns[pat_id] = pres_bin_agreement_pattern(pat_string[0], pat_string[1], pat_string[2], pat_string[3])\n",
    "                pat_string = \"|\".join([\":\".join(i) for i in pat_string])\n",
    "                self.genes_identified[pat_id] = pat_string\n",
    "                self.gene_group[pat_id] = abx\n",
    "\n",
    "\n",
    "# Now with the classes set up we read in everything into an isolates dict\n",
    "isolates = {}\n",
    "\n",
    "for n in tnrange(len(guuids)):\n",
    "    k = guuids[n]\n",
    "    x = isolate(k)\n",
    "    isolates[k] = x\n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "So with everything read in and all the identification patterns produced we next move onto producing files and images for analysis in the rest of the paper.\n",
    "\n",
    "\n",
    "#### AIMS\n",
    "\n",
    "The images we want\n",
    "\n",
    "1. Agreement by the different methods using different databases including\n",
    " * Each of resfinder's specific databases for the 5 gene families of interest\n",
    " * The whole resfinder database\n",
    " * Possibly also adding historical databases.\n",
    " * Making the point that disagreement is highly variable even over a small window of time\n",
    "\n",
    "2. Agreement by the different by different genes, This will include different colors of stacked bar for if only one tool disagreeing + at the top there will be a percentage value of how much of the error is artefactual.\n",
    "\n",
    "The other data we want to analyse but not put into a picture.\n",
    "Table of error suspected causes for each pattern: This would take the form of List the \"pattern\" found and then list which of the common sources of error cause it.\n",
    "How many of the differences affect phneotype\n",
    "\n",
    "\n",
    "\n",
    "#### Necessary outputs to get there\n",
    "\n",
    "This explains the interim files I will want to get to this point\n",
    "\n",
    "Figure 1: This is mostly made manually, but I will create a spreadsheet which demonstrates the numbers involved.\n",
    "\n",
    "Figure 2: So for this I will need to 1. classify each pattern according to the parent \"gene\", 2. Decide if it affects resistance prediction. 3. then decide if it is artefactual using a fairly simple rule framework.\n",
    "\n",
    "To start with we produce useful files which will be key in creating these images\n",
    "\n",
    "**TRG pattern files**\n",
    "For each TRG pattern encountered, I produce a file with how often it was seen\n",
    "This will then be linked with an author provided file (which is produced manually) which derives what the \"gene name\" of the pattern is and whether it would change a predicted phenotype.\n",
    "\n",
    "**Isolate Files**\n",
    "For each isolate, I will produce a file which states the TRG patterns seen in the isolate and whether it was in agreement across methods. This will then be used to define whether the gene level discrepancy is \"simulatable\"\n",
    "To be simulateable it must satisfy the following.\n",
    "1. It must be discrepant\n",
    "2. For at least 1 TRG identified in the discrepancy, it must be contained within the assembly completely on a single contig\n",
    "3. I will finally link it with a file which describes which discrepancies are simulatable or not.\n",
    "4. Note this will produced using code written on analysis rather than code which can be run locally (so as to avoid storing 2000 assemblies locally)\n",
    "5. The code will be put up here but it won't be runable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRG PATTERN FILES\n",
    "\n",
    "# Aggregating the patterns from all samples\n",
    "pattern_counter = {}\n",
    "pattern_bymethod = {}\n",
    "pattern_byabx = {}\n",
    "for i in isolates:\n",
    "    for k in isolates[i].genes_identified:\n",
    "        pattern_key = k + \"|\" + isolates[i].genes_identified[k] \n",
    "        if pattern_key not in pattern_counter.keys():\n",
    "            pattern_bymethod[pattern_key] = \":\".join([str(j) for j in isolates[i].gene_patterns[k]])\n",
    "            pattern_counter[pattern_key] = 1\n",
    "            pattern_byabx[pattern_key] = isolates[i].gene_group[k]\n",
    "        else:\n",
    "            pattern_counter[pattern_key] += 1\n",
    "\n",
    "print(pattern_byabx)\n",
    "# writing this data into a CSV\n",
    "with open(\"by_trg_pattern.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['trg_pattern',\"abx\", \"number_of_isolates\", \"method_agreement\", \"overall_agreement\"])\n",
    "    for key in pattern_counter:\n",
    "        writer.writerow([key,pattern_byabx[key], pattern_counter[key], pattern_bymethod[key], pattern_bymethod[key]==\"1:1:1:1\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ISOLATE files:\n",
    "\n",
    "with open(\"isolate_patterns.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter = \",\")\n",
    "    writer.writerow([\"isolate\", \"pattern\", \"agreement\"])\n",
    "    for k in isolates:\n",
    "        for pat in isolates[k].genes_identified:\n",
    "            writer.writerow([k, pat, (\"0\" not in isolates[k].genes_identified[pat])])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For presentation stuff\n",
    "\n",
    "\n",
    "annotated_patterns = pd.read_csv(\"pattern_annotator.csv\")\n",
    "gene_output = pd.read_csv(\"by_trg_pattern.csv\")\n",
    "pat_data = gene_output.merge(annotated_patterns, on=\"trg_pattern\")\n",
    "simulation_data = pd.read_csv(\"interpreting_simulations.csv\")\n",
    "print(simulation_data.head())\n",
    "\n",
    "def met_agr(l):\n",
    "    if l == \"1:1:1:1\":\n",
    "        return \"0\"\n",
    "    if l == \"1:2:2:2\":\n",
    "        return \"1\"\n",
    "    if l == \"1:2:1:1\":\n",
    "        return \"2\"\n",
    "    if l == \"1:1:2:1\":\n",
    "        return \"3\"    \n",
    "    if l == \"1:1:1:2\":\n",
    "        return \"4\"\n",
    "    else:\n",
    "        return \"5\"\n",
    "\n",
    "pat_dict = {}\n",
    "\n",
    "pg_dict = {}\n",
    "artefact_dict = {}\n",
    "\n",
    "for k in range(len(pat_data)):\n",
    "    k_data = pat_data.iloc[k]\n",
    "    k_pat = k_data.trg_pattern.split(\"|\")[0]\n",
    "    sim_dat = (True in list(simulation_data.loc[simulation_data.pattern == k_pat].overall))\n",
    "    if k_data.method_agreement == \"1:1:1:1\":\n",
    "        k_status = \"0\"\n",
    "    elif sim_dat == True:\n",
    "        k_status = \"1\"\n",
    "    else:\n",
    "        k_status = \"2\"\n",
    "    if k_data.gene_name not in pat_dict.keys():\n",
    "        pat_dict[k_data.gene_name] = {str(i):0 for i in range(6)}\n",
    "        artefact_dict[k_data.gene_name]  = {str(i):0 for i in range(3)}\n",
    "    pat_dict[k_data.gene_name][met_agr(k_data.method_agreement)] += k_data.number_of_isolates\n",
    "    artefact_dict[k_data.gene_name][k_status] += k_data.number_of_isolates\n",
    "    pg_dict[k_data.gene_name] = k_data.abx_x\n",
    "\n",
    "\n",
    "def ad_sum(d):\n",
    "    try:\n",
    "        return d[\"1\"]/(d[\"1\"]+d[\"2\"])\n",
    "    except ZeroDivisionError:\n",
    "        return -1\n",
    "\n",
    "for k in pg_dict:\n",
    "    pg_dict[k] = (pg_dict[k], sum(pat_dict[k].values()),round(ad_sum(artefact_dict[k]), 2))\n",
    "\n",
    "print(pat_dict)\n",
    "for k in sorted(pg_dict.keys(), key = lambda a: (pg_dict[a][0],pg_dict[a][1]) , reverse = True):\n",
    "    print(k, pg_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = plt.figure(figsize=(10, 5), dpi=300)\n",
    "ax1 = plt.subplot2grid((1,1),(0,0), rowspan = 1 , colspan=1)\n",
    "\n",
    "f_keys = [\"blaTEM\", \"blaCTX-M-1\", \"blaOXA-1\",\"blaCMY\", \"blaSHV\",\"blaCTX-M-9\" , \n",
    "         \"aph(6)-Id\",\"aph(3'')-Ib\", \"ant(3'')-Ia\", \"aadA5\", \"aac(3)-IIa\", \"aac(6')-Ib\", \"aph(3')-Ia\", \n",
    "         \"aac(3)-IV\", \"aph(4)-Ia\", \n",
    "         \"qnrS\",\n",
    "         \"dfrA7\",\"dfrA1\", \"drfA12\", \"dfrA14\" , \"dfrA5\", \n",
    "         \"sul2\",\"sul1\", \"sul3\" ]\n",
    "xs = range(len(f_keys))\n",
    "f_0vals = [pat_dict[k]['0'] for k in f_keys]\n",
    "f_1vals = [pat_dict[k]['1'] for k in f_keys]\n",
    "f_2vals = [pat_dict[k]['2'] for k in f_keys]\n",
    "f_3vals = [pat_dict[k]['3'] for k in f_keys]\n",
    "f_4vals = [pat_dict[k]['4'] for k in f_keys]\n",
    "f_5vals = [pat_dict[k]['5'] for k in f_keys]\n",
    "def convert_numbers(l):\n",
    "    out_list = []\n",
    "    for k in l:\n",
    "        if k != -1:\n",
    "            out_list.append(str(int(100*k)) + \"%\")\n",
    "        else:\n",
    "            out_list.append(\"N/A\")\n",
    "    return out_list\n",
    "\n",
    "f_numbers = convert_numbers([pg_dict[k][2] for k in f_keys])\n",
    "print(f_numbers)\n",
    "width = 0.5\n",
    "\n",
    "ax1.bar(xs, f_0vals, width)\n",
    "ax1.bar(xs, f_1vals, width,\n",
    "             bottom=f_0vals, label=\"ABRicate discrepant\")\n",
    "ax1.bar(xs, f_2vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i] for i in range(len(f_0vals))], label=\"ARIBA discrepant\")\n",
    "ax1.bar(xs, f_3vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i]+f_2vals[i] for i in range(len(f_0vals))], label=\"KmerResistance discrepant\")\n",
    "ax1.bar(xs, f_4vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i]+f_2vals[i]+f_3vals[i] for i in range(len(f_0vals))], label=\"SRST2 discrepant\")\n",
    "ax1.bar(xs, f_5vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i]+f_2vals[i]+f_3vals[i]+f_4vals[i] for i in range(len(f_0vals))], label=\"Multiple discrepant\")\n",
    "\n",
    "ax1.set_xticks(range(len(f_keys)))\n",
    "ax1.set_xticklabels(f_keys, rotation=90)\n",
    "ax1.set_yticklabels([], rotation=90)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
