{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: blue;\">Analysing method related differences script - bioinformatic differences</span>\n",
    "\n",
    "This script goes through the investigating discrepancies between the 4 methods\n",
    "\n",
    "1. ABRicate (our baseline method)\n",
    "2. ARIBA\n",
    "3. KmerResistance\n",
    "4. SRST2\n",
    "\n",
    "**NOTE FOR EASE OF READING, SOME CODE IS REPEATED IN EACH SECTION, TO MAKE EACH SECTION BE A DISCREET UNIT OF ANALYSIS, THIS LENGTHANS THE RUNTIME OF THE CODE BUT MAKES IT EASIER TO UNDERSTAND. THE RUNTIME IS STILL SHORT (< 2 hours even comparing all database similarities)**\n",
    "\n",
    "### <span style=\"color: green;\">Highlighting database dependency.</span>\n",
    "\n",
    "For each of several databases, it does the following:\n",
    "1. reads in all the results we obtained\n",
    "3. This is then used to produce **Figure X** \n",
    "\n",
    "\n",
    "Databases used\n",
    "**Primary**\n",
    "Resfinder 1st October 2019 release \n",
    "**Secondary**\n",
    "* CARD 23rd October 2019 release\n",
    "* beta-lactam.fsa of the Resfinder 1st October 2019 release\n",
    "* aminoglycoside.fsa  of the Resfinder 1st October 2019 release\n",
    "* quinolone.fsa of the Resfinder 1st October 2019 release\n",
    "* trimethoprim.fsa of the Resfinder 1st October 2019 release\n",
    "* sulphonamide.fsa of the Resfinder 1st October 2019 release\n",
    "* Resfinder 22nd of January 2019 release\n",
    "* Resfinder 22nd of January 2018 release\n",
    "* Resfinder 26th of January 2017 release\n",
    "\n",
    "\n",
    "\n",
    "From here on, we are investigating causes of error and only use the Resfinder 1st October 2019 release.\n",
    "\n",
    "### <span style=\"color: green;\">Identifying which TRGs are most commonly discrepant and then </span>\n",
    "\n",
    "1. For this we first identify which reported TRGs likely represent the same gene found differently by the different programs (see image below)\n",
    "\n",
    "![image](method_1.png)\n",
    "\n",
    "2. We then look at patterns of discrepancy and identify the 10 most common across the data\n",
    "3. We then demonstrate what we believe the cause of each of these is (simulated example in each cell of the notebook)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color: green;\">Highlighting extent of annotation errors</span>\n",
    "\n",
    "This then goes through how we determine if an error is likely due to annotation or not\n",
    "\n",
    "1. For each discrepant, we identify if any of the identified TRGs are complete within the assembly.\n",
    "2. If so, we take the containing contig and simulate **perfect* reads at 50x and 500x from it (10 times each).\n",
    "3. We then observe if the same misclassification pattern occurs again. If so the error is deemed likely annotation\n",
    "4. This is then used to produce **Figure XX** in the paper.\n",
    "\n",
    "Note for some of the code from this section is only presented here in markdown as I can't provide any full sequence files on GitHub. However, the sequences for this project are available on NCBI and the code can be recreated to analyse these scripts here.\n",
    "\n",
    "### <span style=\"color: green;\">Investigating non annotation errors.</span>\n",
    "\n",
    "This section finally ends on investigation of 6 samples where different beta-lcatamase genes were reported but were not artefactual.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Setup</span>\n",
    "\n",
    "**So first steps are to load in required modules and then identify all the output reports**\n",
    "\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "1. Python 3 \n",
    "2. Biopython\n",
    "3. Pandas\n",
    "4. Numpy\n",
    "5. tqdm\n",
    "6. networkx\n",
    "\n",
    "#### Inputs\n",
    "Some notes for this step, firstly which files we take\n",
    "1. for ABRicate we take each contigs.tab file\n",
    "2. for ARIBA we use its summary file\n",
    "3. for KmerResistance we use the .KmerRes files\n",
    "4. for SRST2 we use the .out__fullgenes__seqs_clustered__results.txt files\n",
    "\n",
    "Note these were chosen as they seem to follow guidelines and where no guidelines available, give us the most closely matching results between the four programs\n",
    "\n",
    "Note also SRST2 does not produce output if it finds no genes. The others all do\n",
    "\n",
    "#### Resfinder database\n",
    "\n",
    "For this we load\n",
    "1. The naming link database\n",
    "2. For each of the sub databases by antibiotic class \n",
    "  * beta-lactam\n",
    "  * quinolone\n",
    "  * aminoglycoside\n",
    "  * sulfa antibiotics\n",
    "  * trimethoprim\n",
    "3. The whole database\n",
    "\n",
    "We do 1 so the results are interpretable\n",
    "We do 2 so we can breakdown results by antibiotic class as well as specific genes\n",
    "We do 3 to do all versus all of the resistance database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by_trg_pattern.csv\r\n",
      "interpreting_simulations.csv\r\n",
      "isolate_patterns.csv\r\n",
      "\u001b[1m\u001b[36mlegacy_code\u001b[m\u001b[m/\r\n",
      "main_analysis.ipynb\r\n",
      "method_1.png\r\n",
      "method_1.svg\r\n",
      "method_combinations.png\r\n",
      "method_image.png\r\n",
      "notes_random.txt\r\n",
      "pattern_annotator.csv\r\n",
      "readymade_card_20191023_full_sim_matrix.csv\r\n",
      "readymade_resfinder_20170126_full_sim_matrix.csv\r\n",
      "readymade_resfinder_20180122_full_sim_matrix.csv\r\n",
      "readymade_resfinder_20190122_full_sim_matrix.csv\r\n",
      "readymade_resfinder_20191001_ami_sim_matrix.csv\r\n",
      "readymade_resfinder_20191001_blm_sim_matrix.csv\r\n",
      "readymade_resfinder_20191001_full_sim_matrix.csv\r\n",
      "readymade_resfinder_20191001_qui_sim_matrix.csv\r\n",
      "readymade_resfinder_20191001_sul_sim_matrix.csv\r\n",
      "readymade_resfinder_20191001_tri_sim_matrix.csv\r\n",
      "\u001b[1m\u001b[36mresult_tarballs\u001b[m\u001b[m/\r\n",
      "simulation_analysis.ipynb\r\n",
      "simulation_contigs.csv\r\n"
     ]
    }
   ],
   "source": [
    "# We start by importing required modules\n",
    "# File structure\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# These are the fundamental modules used for analysing the data\n",
    "\n",
    "\n",
    "# Pictures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sequence manipulation\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "# These modules are priarily used for loading the database, and then for identifying whether there are any\n",
    "# Perfrect protein matches in the dataset\n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "\n",
    "# Other general\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "# We just import this to check our code runs sensibly and to get timing estimates for stuff\n",
    "from copy import deepcopy\n",
    "\n",
    "#Looking at local files\n",
    "%ls \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Loading results</span>\n",
    "\n",
    "The following block of code finds all the result files and puts them together into a single dictionary\n",
    "Note we re-ran each program several times using several different databases. Most of these results aren't in the main text (aside from those pertaining to the 20191001 database) but refer to tings in the supplementary.\n",
    "Also note we re-ran each database on the sub-databases of interest to make sure different sub-databases didn't interfere with one another.\n",
    "\n",
    "### Results all contained within the result_tarballs directory and dbs all contained within the ../db_preparation directory \n",
    "\n",
    "Following dictionary links tarballs (for results), and database files, so that all can be analysed together\n",
    "All tarballs available in result_tarballs. please run tar -xzvf * on all tarballs prior to running this code\n",
    "Note we don't provide all the results as the files become large and difficult to work with in this small repo\n",
    "Instead we just keep the files we need. These can all be reproduced by accding the sequence files directly and \n",
    "Then reruning each piece of software using the databases provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Giving it where files are, note if you unpack the result tarballs somwhere else you will need to repoint the \n",
    "# \"results\" files\n",
    "\n",
    "dbresult_link = {\"resfinder_20191001_full\":{\"results\":\"result_tarballs/resfinderfull_20191001/\",\n",
    "                                          \"db\": \"../db_preparation/resfinder_20191001/\"},\n",
    "                 \"resfinder_20191001_blm\":{\"results\":\"result_tarballs/resfinderblm_20191001/\",\n",
    "                                          \"db\":\"../db_preparation/resfinder_20191001_blm/\"},\n",
    "                 \"resfinder_20191001_ami\":{\"results\":\"result_tarballs/resfinderami_20191001/\",\n",
    "                                          \"db\":\"../db_preparation/resfinder_20191001_ami/\"},\n",
    "                 \"resfinder_20191001_qui\":{\"results\":\"result_tarballs/resfinderqui_20191001/\",\n",
    "                                           \"db\":\"../db_preparation/resfinder_20191001_qui/\"},\n",
    "                 \"resfinder_20191001_tri\":{\"results\":\"result_tarballs/resfindertri_20191001/\", \n",
    "                                          \"db\":\"../db_preparation/resfinder_20191001_tri/\"},\n",
    "                 \"resfinder_20191001_sul\":{\"results\":\"result_tarballs/resfindersul_20191001/\", \n",
    "                                          \"db\":\"../db_preparation/resfinder_20191001_sul/\"},\n",
    "                 \"resfinder_20190122_full\":{\"results\":\"result_tarballs/resfinder_20190122/\", \n",
    "                                          \"db\":\"../db_preparation/resfinder_20190122/\"},\n",
    "                 \"resfinder_20180122_full\":{\"results\":\"result_tarballs/resfinder_20180122/\", \n",
    "                                          \"db\":\"../db_preparation/resfinder_20180122/\"},\n",
    "                 \"resfinder_20170126_full\":{\"results\":\"result_tarballs/resfinder_20170126/\", \n",
    "                                          \"db\":\"../db_preparation/resfinder_20170126/\"},\n",
    "                 \"card_20191023_full\":{\"results\":\"result_tarballs/card_20191023/\", \n",
    "                                       \"db\":\"../db_preparation/card_20191023/\"}\n",
    "            }\n",
    "\n",
    "\n",
    "# Loading in the output files\n",
    "\n",
    "output_files = {}\n",
    "\n",
    "for k in dbresult_link:\n",
    "    output_files[k] = {}\n",
    "    base_key = dbresult_link[k]['results'] + dbresult_link[k]['results'].split(\"/\")[-2] + \"_\"\n",
    "    # loading in ABRicate first\n",
    "    output_files[k][\"abricate\"] = base_key + \"abricate/\"\n",
    "    abricate_files = [os.path.join(root, f) for root, dirs, files \n",
    "                  in os.walk(output_files[k][\"abricate\"])\n",
    "                 for f in files if f != \"summary.tab\"]\n",
    "    abricate_files = {k.split(\"/\")[-1].split(\"_\")[0]:k for k in abricate_files}\n",
    "    output_files[k][\"abricate\"] = abricate_files\n",
    "    # Loading the KmerRes files\n",
    "    output_files[k][\"kmerres\"] = base_key + \"kmerres/\"\n",
    "    kmerres_files = [os.path.join(root, f) for root, dirs, files in os.walk(output_files[k][\"kmerres\"])\n",
    "                     for f in files if \".KmerRes\" in f]\n",
    "    kmerres_files = {k.split(\"/\")[-1].split(\"_\")[0]:k for k in kmerres_files}    \n",
    "    output_files[k][\"kmerres\"] = kmerres_files\n",
    "    # The SRST2 files\n",
    "    output_files[k]['srst2'] = base_key + \"srst2/\"\n",
    "    srst2_files = [os.path.join(root, f) for root, dirs, files in os.walk(output_files[k]['srst2']) for f in files]\n",
    "    srst2_files = {k.split(\"/\")[-1].split(\"_\")[0]:k for k in srst2_files}\n",
    "    output_files[k]['srst2'] = srst2_files\n",
    "    # Finally we'll put the whole ariba summary into a pandas database\n",
    "    output_files[k][\"ariba\"] = base_key + \"ariba.csv\"\n",
    "    ariba_names = [k.split(\"/\")[1].split(\"_\")[0] for k in list(pd.read_csv(output_files[k][\"ariba\"]).name)]\n",
    "    ariba_summary = pd.read_csv(output_files[k][\"ariba\"]).fillna(\"\")\n",
    "    ariba_summary.index = ariba_names\n",
    "    output_files[k][\"ariba\"] = ariba_summary\n",
    "    \n",
    "\n",
    "\n",
    "# Note because of the different way in which ARIBA is loaded in we also add a special reading function\n",
    "def ariba_parser(s):\n",
    "    s_clusters = sorted(list(set([k.split(\".\")[0] for k in s.index if \"cluster\" in k and \".match\" in k])))\n",
    "    s_clusters = [k for k in s_clusters if s[k+\".match\"] == \"yes\"]\n",
    "    s_genes = [s[k+ \".ref_seq\"] for k in s_clusters]\n",
    "    return s_genes\n",
    "\n",
    "guuids = list(output_files[\"resfinder_20191001_full\"][\"abricate\"].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on in I am going to focus on an individual database, but this is specified at the start\n",
    " \n",
    "1. **resfinder_20191001_full** - The full 1st October 2019 Resfinder database\n",
    "2. **resfinder_20191001_blm** - The beta-lactams only from the 1st October 2019 Resfinder database\n",
    "3. **resfinder_20191001_ami** - The aminoglycosides only from the 1st October 2019 Resfinder database\n",
    "4. **resfinder_20191001_qui** - The quinolones only from the 1st October 2019 Resfinder database\n",
    "5. **resfinder_20191001_sul** - The sulphonomides only from the 1st October 2019 Resfinder database\n",
    "6. **resfinder_20191001_tri** - The trimethoprim only from the 1st October 2019 Resfinder database\n",
    "7. **resfinder_20190122_full** - The full 22nd Jan 2019 Resfinder database\n",
    "8. **resfinder_20180122_full** - The full 22nd Jan 2018 Resfinder database\n",
    "9. **resfinder_20170126_full** - The full 26nd Jan 2017 Resfinder database\n",
    "10. **card_20191023_full** - The full 23rd October CARD database\n",
    "\n",
    "Within these files (see above dict) there is a standard file structure\n",
    "To use a different database, you just select a different database\n",
    "Note for code which only applies to the resfinder_20191001_full database (The main database used in the paper) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "poss_dbs = [\"resfinder_20191001_full\", \"resfinder_20191001_blm\", \"resfinder_20191001_ami\", \n",
    "           \"resfinder_20191001_qui\", \"resfinder_20191001_sul\", \"resfinder_20191001_tri\",\"resfinder_20190122_full\", \n",
    "           \"resfinder_20180122_full\",\"resfinder_20170126_full\", \"card_20191023_full\"]\n",
    "\n",
    "\n",
    "db_choice = \"resfinder_20191001_full\"\n",
    "# Database\n",
    "db = dbresult_link[db_choice][\"db\"]\n",
    "formatted_db = db +\"db_formatted.fasta\"\n",
    "linkfile = db + \"db_link.csv\"\n",
    "\n",
    "#results\n",
    "\n",
    "abricate_files = output_files[db_choice]['abricate']\n",
    "kmerres_files = output_files[db_choice]['kmerres']\n",
    "srst2_files = output_files[db_choice]['srst2']\n",
    "ariba_summary = output_files[db_choice]['ariba']\n",
    "\n",
    "\n",
    "# 1. loading the link file\n",
    "link = pd.read_csv(linkfile, index_col=0, header=None)\n",
    "rlink = pd.read_csv(linkfile, index_col=1, header=None)\n",
    "\n",
    "\n",
    "# 2. loading the atabase\n",
    "res_db = SeqIO.to_dict(SeqIO.parse(formatted_db, \"fasta\"))\n",
    "\n",
    "# 3. Next were going to do all vs all similarity of the resfinder database\n",
    "\n",
    "sim_matrix = pd.DataFrame(np.zeros((len(res_db.keys()),len(res_db.keys()) )), \n",
    "                         columns = sorted(list(res_db.keys())), index=sorted(list(res_db.keys())))\n",
    "# For this similarity we will use 17-mers (one of the prefixes using MASH, note several others were checked prior to this for their effects)\n",
    "# Clusters marked by this are fairly similar to other k-mer sizes\n",
    "res_db_kmers = {k: set([str(res_db[k].seq)[i:i+17] for i in range(len(res_db[k].seq)-16)]) for k in res_db.keys()}\n",
    "\n",
    "def calculate_jac_sim(l1, l2):\n",
    "    intersection = len(res_db_kmers[l1].intersection(res_db_kmers[l2]))\n",
    "    union = len(res_db_kmers[l1].union(res_db_kmers[l2]))\n",
    "    return(intersection/union)\n",
    "\n",
    "# This code actually goes through populating this matrix, \n",
    "# However takes 30 minutes to run, and for the sake of running this code quickly on line, I will\n",
    "# use a matrix I made earlier (using thie code)\n",
    "if os.path.isfile(\"readymade\"+ \"_{0}_sim_matrix.csv\" .format(db_choice)) == False:\n",
    "    for n in tnrange(len(res_db_kmers)):\n",
    "        k  = list(res_db_kmers.keys())[n]\n",
    "        for j in res_db_kmers:\n",
    "            sim_matrix.loc[k][j] = calculate_jac_sim(k, j)\n",
    "    sim_matrix.to_csv(\"readymade\"+ \"_{0}_sim_matrix.csv\" .format(db_choice))\n",
    "\n",
    "sim_matrix = pd.read_csv(\"readymade\"+ \"_{0}_sim_matrix.csv\" .format(db_choice), index_col = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the analysis class\n",
    "\n",
    "\n",
    "In the next section of code, the aim is to define a class which performs most of the comparisons for us. \n",
    "\n",
    "#### Defining useful functions\n",
    "Before we set up the class we define useful functions\n",
    "There are some general useful functions and also some more specific ones. \n",
    "The specific and less obvious ones are below.\n",
    "\n",
    "**CLUSTERING**\n",
    "\n",
    "recursive cluster => This essentially uses netrowkx to greate a graph, which links togehter elements with non-zero similarity. \n",
    "The other two functions make_tuples and name list are simplications of bits within the recursive cluster functions\n",
    "\n",
    "**AGREEMENT**\n",
    "\n",
    "We also use a function to define agreement\n",
    "This is then useful for quick and easy ready of \n",
    "which programs have agreed together. \n",
    "Note the panel is the same one as included in a supplementary image.\n",
    "\n",
    "![image](method_combinations.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Reading in the data\n",
    "Once we have the tools to analyse the data we actually read in the data\n",
    "\n",
    "This does the following steps\n",
    "For each of ABRicate, ARIBA, KmerResistance, SRST2 we\n",
    "1. read in its file\n",
    "2. Pull out the TRGs it identifies and relabel them with their original names\n",
    "3. Separate these into groups according to their relavent antibiotics.\n",
    "\n",
    "\n",
    "\n",
    "#### Comparing outputs\n",
    "\n",
    "The next stage is to define what genes we have in the sample, here we then first\n",
    "1. make a combined dict of all genes found. \n",
    "2. Then separate according to clusters\n",
    "3. make identification patterns for each of these clusters\n",
    "\n",
    "\n",
    "\n",
    "This is done using an external spreadsheet (which suggests putative families for all patterns of genes seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### USEFUL FUNCTIONS ######\n",
    "\n",
    "\n",
    "###### CLUSTERING FUNCTIONS  ######\n",
    "\n",
    "\n",
    "def make_tuples(l):\n",
    "    # Make all possible tuples in a list\n",
    "    output = []\n",
    "    for i in range(len(l)):\n",
    "        for j in range(len(l)):\n",
    "            output.append((l[i],l[j]))\n",
    "    output = sorted(list(set(output)))\n",
    "    return output\n",
    "\n",
    "def name_list(l, d):\n",
    "    #rename all elements of a list\n",
    "    return [d[k] for k in l]\n",
    "\n",
    "\n",
    "def recursive_cluster(df, l):\n",
    "    groups = {}\n",
    "    # First we get all linked pais.\n",
    "    for i in l:\n",
    "        i_data = df.loc[i]\n",
    "        i_group = [i]\n",
    "        for j in l:\n",
    "            if j != i: \n",
    "                if i_data[j] != 0:\n",
    "                    i_group.append(j)\n",
    "        i_group = sorted(i_group)\n",
    "        groups[i] =  i_group\n",
    "    # Assign numbers to the elements of l and then generate a dictionary to link numbers and names\n",
    "    naming = {}\n",
    "    reverse_naming  = {}\n",
    "    m = 1\n",
    "    for i in l:\n",
    "        naming[i] = m\n",
    "        reverse_naming[m] = i\n",
    "        m += 1\n",
    "    # Grouping tuples like a graph using networkx\n",
    "    final_tuples = []\n",
    "    for i in groups:\n",
    "        final_tuples = final_tuples + make_tuples([naming[j] for j in groups[i]])\n",
    "    final_tuples = sorted(list(set(final_tuples)))\n",
    "    graph=nx.Graph(final_tuples)\n",
    "    output = [name_list(list(c), reverse_naming) for c in nx.connected_components(graph)]\n",
    "    return output\n",
    "\n",
    "###### AGREEMENT PATTERN FUNCTIONS \n",
    "\n",
    "# Note for these functions they always assume the results are put in the correct order\n",
    "# i.e. ABRicate, ARIBA, KmerResistance, SRST2\n",
    "\n",
    "# First we start with a general agreement function \n",
    "def agreement_pattern(l1, l2, l3, l4):\n",
    "    args = deepcopy(locals())\n",
    "    arg_list = ['l1', 'l2', 'l3', 'l4']\n",
    "    for key in arg_list:\n",
    "        args[key] = \":\".join(sorted(args[key]))\n",
    "    patterns = {}\n",
    "    output = []\n",
    "    starting_no = 0\n",
    "    for key in arg_list:\n",
    "        if args[key] not in patterns:\n",
    "            starting_no += 1\n",
    "            patterns[args[key]] = starting_no\n",
    "            output.append(starting_no)\n",
    "        else:\n",
    "            output.append(patterns[args[key]])\n",
    "    return output\n",
    "    \n",
    "\n",
    "# Now for gene agreement, I use this program to say which genes (from a list) each method has found\n",
    "def pres_bin(l1, l2):\n",
    "    output = []\n",
    "    for k in l1:\n",
    "        if k in l2:\n",
    "            output.append(\"1\")\n",
    "        else:\n",
    "            output.append(\"0\")\n",
    "    return output\n",
    "\n",
    "# Here is the agreement function again, but this time i've dropped the sort function. \n",
    "# This enables me to use the output from pres_bin directly to make the patterns as defined above \n",
    "def pres_bin_agreement_pattern(l1, l2, l3, l4):\n",
    "    args = deepcopy(locals())\n",
    "    arg_list = ['l1', 'l2', 'l3', 'l4']\n",
    "    for key in arg_list:\n",
    "        args[key] = \":\".join(args[key])\n",
    "    patterns = {}\n",
    "    output = []\n",
    "    starting_no = 0\n",
    "    for key in arg_list:\n",
    "        if args[key] not in patterns:\n",
    "            starting_no += 1\n",
    "            patterns[args[key]] = starting_no\n",
    "            output.append(starting_no)\n",
    "        else:\n",
    "            output.append(patterns[args[key]])\n",
    "    return output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddc25b02c2e409e95b758e830edab0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1818), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading in each of the data\n",
    "\n",
    "\n",
    "\n",
    "class isolate:\n",
    "    \n",
    "    def __init__(self,guuid):\n",
    "        self.guuid = guuid\n",
    "        # First lest start with ABRicate, for this one we take everything assuming control\n",
    "        # Each section of this code does similar things, 1. read the file , 2 translate the genes, \n",
    "        self.abricate_fl = pd.read_csv(abricate_files[self.guuid], delimiter= \"\\t\").fillna(\"\")\n",
    "        self.abricate_fl = self.abricate_fl.loc[self.abricate_fl['%COVERAGE'] > 60.0]\n",
    "        self.abricate_fl = self.abricate_fl.loc[self.abricate_fl['%IDENTITY'] > 90.0]\n",
    "        self.abricate_genes = sorted(list(set([link.loc[k][1] for k in list(self.abricate_fl['GENE'])])))\n",
    "\n",
    "        \n",
    "        # Next ARIBA\n",
    "        self.ariba_data = ariba_parser(ariba_summary.loc[self.guuid])\n",
    "        self.ariba_genes = sorted(list(set([link.loc[k][1] for k in self.ariba_data])))\n",
    "\n",
    "        # Next KmerResistance\n",
    "        # For this one we also add in a coverage cut-off given our file doesn't seem to be able to do this reliably\n",
    "        # Plus we are trying to apply the 70% cutoff as it doesn't work easily in the coverage\n",
    "        # So we will re-apply this.)\n",
    "        self.kmerres_fl = pd.read_csv(kmerres_files[self.guuid], delimiter = \"\\t\").fillna(\"\")\n",
    "        self.kmerres_fl = self.kmerres_fl.loc[self.kmerres_fl.template_id > 70.0]\n",
    "        self.kmerres_genes = sorted(list(set([link.loc[k][1] for k in [j for j in list(self.kmerres_fl['#Template']) if \"resfindernewid\" in j]])))\n",
    "\n",
    "        # Note for SRST2 we have another bit which doesn't quite work\n",
    "        # It does not make a file if it finds no genes\n",
    "        # Therefore we put it into a try except group\n",
    "\n",
    "        try:\n",
    "            self.srst2_fl = pd.read_csv(srst2_files[self.guuid], delimiter = \"\\t\").fillna(\"\")\n",
    "            self.srst2_genes = sorted(list(set([link.loc[k][1] for k in list(self.srst2_fl['allele'])])))\n",
    "        except:\n",
    "            self.srst2_fl = \"N/A\"\n",
    "            self.srst2_genes = []\n",
    "\n",
    "        \n",
    "        ### Aggregating genes. \n",
    "        \n",
    "        self.geno_full = {\"abricate\":self.abricate_genes, \"ariba\":self.ariba_genes, \n",
    "                         \"kmerres\": self.kmerres_genes, \"srst2\":self.srst2_genes}\n",
    "        self.all_genes = sorted(list(set(self.abricate_genes + self.ariba_genes + self.srst2_genes + self.kmerres_genes)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Defining gene families \n",
    "\n",
    "        gene_df = pd.DataFrame(np.zeros((len(self.all_genes),len(self.all_genes) )), \n",
    "                               columns =self.all_genes, index=self.all_genes)\n",
    "        for l in gene_df.index:\n",
    "            for j in gene_df.columns:\n",
    "                gene_df.loc[l][j] = sim_matrix.loc[rlink.loc[l][0]][rlink.loc[j][0]]\n",
    "        self.gene_families = recursive_cluster(gene_df, gene_df.index)\n",
    "            \n",
    "            \n",
    "#         ### Assessing levels of agreement\n",
    "#         # Here we define three things, Firstly, do results agree for all genes for a particular antibiotic class\n",
    "#         # Then do they agree for a whole isolate\n",
    "#         # Then finally we do a bit more delving into the patterns of disagreement\n",
    "#         # Whole isolate level agreement\n",
    "        self.isolate_patterns = agreement_pattern(sorted(self.abricate_genes), sorted(self.ariba_genes),\n",
    "                                                sorted(self.kmerres_genes), sorted(self.srst2_genes))\n",
    "        self.isolate_agreement = (self.isolate_patterns == [1,1,1,1])\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "#         ###For each gene\n",
    "        self.genes_identified = {}\n",
    "        self.gene_patterns = {}\n",
    "        for pat in self.gene_families:\n",
    "            pat_id = \":\".join(pat)\n",
    "            pat_string = [pres_bin(pat, self.abricate_genes),\n",
    "                          pres_bin(pat, self.ariba_genes), \n",
    "                          pres_bin(pat, self.kmerres_genes), \n",
    "                          pres_bin(pat, self.srst2_genes)]\n",
    "            self.gene_patterns[pat_id] = pres_bin_agreement_pattern(pat_string[0], pat_string[1], pat_string[2], pat_string[3])\n",
    "            pat_string = \"|\".join([\":\".join(i) for i in pat_string])\n",
    "            self.genes_identified[pat_id] = pat_string\n",
    "\n",
    "\n",
    "\n",
    "# Now with the classes set up we read in everything into an isolates dict\n",
    "isolates = {}\n",
    "\n",
    "for n in tnrange(len(guuids)):\n",
    "    k = guuids[n]\n",
    "    x = isolate(k)\n",
    "    isolates[k] = x\n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "So with everything read in and all the identification patterns produced we next move onto producing files and images for analysis in the rest of the paper.\n",
    "\n",
    "\n",
    "#### AIMS\n",
    "\n",
    "The images we want\n",
    "\n",
    "1. Agreement by the different methods using different databases including\n",
    " * Each of resfinder's specific databases for the 5 gene families of interest\n",
    " * The whole resfinder database\n",
    " * Possibly also adding historical databases.\n",
    " * Making the point that disagreement is highly variable even over a small window of time\n",
    "\n",
    "2. Agreement by the different by different genes, This will include different colors of stacked bar for if only one tool disagreeing + at the top there will be a percentage value of how much of the error is artefactual.\n",
    "\n",
    "The other data we want to analyse but not put into a picture.\n",
    "Table of error suspected causes for each pattern: This would take the form of List the \"pattern\" found and then list which of the common sources of error cause it.\n",
    "How many of the differences affect phneotype\n",
    "\n",
    "\n",
    "\n",
    "#### Necessary outputs to get there\n",
    "\n",
    "This explains the interim files I will want to get to this point\n",
    "\n",
    "Figure 1: This is mostly made manually, but I will create a spreadsheet which demonstrates the numbers involved.\n",
    "\n",
    "Figure 2: So for this I will need to 1. classify each pattern according to the parent \"gene\", 2. Decide if it affects resistance prediction. 3. then decide if it is artefactual using a fairly simple rule framework.\n",
    "\n",
    "To start with we produce useful files which will be key in creating these images\n",
    "\n",
    "**TRG pattern files**\n",
    "For each TRG pattern encountered, I produce a file with how often it was seen\n",
    "This will then be linked with an author provided file (which is produced manually) which derives what the \"gene name\" of the pattern is and whether it would change a predicted phenotype.\n",
    "\n",
    "**Isolate Files**\n",
    "For each isolate, I will produce a file which states the TRG patterns seen in the isolate and whether it was in agreement across methods. This will then be used to define whether the gene level discrepancy is \"simulatable\"\n",
    "To be simulateable it must satisfy the following.\n",
    "1. It must be discrepant\n",
    "2. For at least 1 TRG identified in the discrepancy, it must be contained within the assembly completely on a single contig\n",
    "3. I will finally link it with a file which describes which discrepancies are simulatable or not.\n",
    "4. Note this will produced using code written on analysis rather than code which can be run locally (so as to avoid storing 2000 assemblies locally)\n",
    "5. The code will be put up here but it won't be runable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRG PATTERN FILES\n",
    "\n",
    "# Aggregating the patterns from all samples\n",
    "pattern_counter = {}\n",
    "pattern_bymethod = {}\n",
    "pattern_byabx = {}\n",
    "for i in isolates:\n",
    "    for k in isolates[i].genes_identified:\n",
    "        pattern_key = k + \"|\" + isolates[i].genes_identified[k] \n",
    "        if pattern_key not in pattern_counter.keys():\n",
    "            pattern_bymethod[pattern_key] = \":\".join([str(j) for j in isolates[i].gene_patterns[k]])\n",
    "            pattern_counter[pattern_key] = 1\n",
    "            pattern_byabx[pattern_key] = isolates[i].gene_group[k]\n",
    "        else:\n",
    "            pattern_counter[pattern_key] += 1\n",
    "\n",
    "print(pattern_byabx)\n",
    "# writing this data into a CSV\n",
    "with open(\"by_trg_pattern.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['trg_pattern',\"abx\", \"number_of_isolates\", \"method_agreement\", \"overall_agreement\"])\n",
    "    for key in pattern_counter:\n",
    "        writer.writerow([key,pattern_byabx[key], pattern_counter[key], pattern_bymethod[key], pattern_bymethod[key]==\"1:1:1:1\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ISOLATE files:\n",
    "\n",
    "with open(\"isolate_patterns.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter = \",\")\n",
    "    writer.writerow([\"isolate\", \"pattern\", \"agreement\"])\n",
    "    for k in isolates:\n",
    "        for pat in isolates[k].genes_identified:\n",
    "            writer.writerow([k, pat, (\"0\" not in isolates[k].genes_identified[pat])])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For presentation stuff\n",
    "\n",
    "\n",
    "annotated_patterns = pd.read_csv(\"pattern_annotator.csv\")\n",
    "gene_output = pd.read_csv(\"by_trg_pattern.csv\")\n",
    "pat_data = gene_output.merge(annotated_patterns, on=\"trg_pattern\")\n",
    "simulation_data = pd.read_csv(\"interpreting_simulations.csv\")\n",
    "print(simulation_data.head())\n",
    "\n",
    "def met_agr(l):\n",
    "    if l == \"1:1:1:1\":\n",
    "        return \"0\"\n",
    "    if l == \"1:2:2:2\":\n",
    "        return \"1\"\n",
    "    if l == \"1:2:1:1\":\n",
    "        return \"2\"\n",
    "    if l == \"1:1:2:1\":\n",
    "        return \"3\"    \n",
    "    if l == \"1:1:1:2\":\n",
    "        return \"4\"\n",
    "    else:\n",
    "        return \"5\"\n",
    "\n",
    "pat_dict = {}\n",
    "\n",
    "pg_dict = {}\n",
    "artefact_dict = {}\n",
    "\n",
    "for k in range(len(pat_data)):\n",
    "    k_data = pat_data.iloc[k]\n",
    "    k_pat = k_data.trg_pattern.split(\"|\")[0]\n",
    "    sim_dat = (True in list(simulation_data.loc[simulation_data.pattern == k_pat].overall))\n",
    "    if k_data.method_agreement == \"1:1:1:1\":\n",
    "        k_status = \"0\"\n",
    "    elif sim_dat == True:\n",
    "        k_status = \"1\"\n",
    "    else:\n",
    "        k_status = \"2\"\n",
    "    if k_data.gene_name not in pat_dict.keys():\n",
    "        pat_dict[k_data.gene_name] = {str(i):0 for i in range(6)}\n",
    "        artefact_dict[k_data.gene_name]  = {str(i):0 for i in range(3)}\n",
    "    pat_dict[k_data.gene_name][met_agr(k_data.method_agreement)] += k_data.number_of_isolates\n",
    "    artefact_dict[k_data.gene_name][k_status] += k_data.number_of_isolates\n",
    "    pg_dict[k_data.gene_name] = k_data.abx_x\n",
    "\n",
    "\n",
    "def ad_sum(d):\n",
    "    try:\n",
    "        return d[\"1\"]/(d[\"1\"]+d[\"2\"])\n",
    "    except ZeroDivisionError:\n",
    "        return -1\n",
    "\n",
    "for k in pg_dict:\n",
    "    pg_dict[k] = (pg_dict[k], sum(pat_dict[k].values()),round(ad_sum(artefact_dict[k]), 2))\n",
    "\n",
    "print(pat_dict)\n",
    "for k in sorted(pg_dict.keys(), key = lambda a: (pg_dict[a][0],pg_dict[a][1]) , reverse = True):\n",
    "    print(k, pg_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = plt.figure(figsize=(10, 5), dpi=300)\n",
    "ax1 = plt.subplot2grid((1,1),(0,0), rowspan = 1 , colspan=1)\n",
    "\n",
    "f_keys = [\"blaTEM\", \"blaCTX-M-1\", \"blaOXA-1\",\"blaCMY\", \"blaSHV\",\"blaCTX-M-9\" , \n",
    "         \"aph(6)-Id\",\"aph(3'')-Ib\", \"ant(3'')-Ia\", \"aadA5\", \"aac(3)-IIa\", \"aac(6')-Ib\", \"aph(3')-Ia\", \n",
    "         \"aac(3)-IV\", \"aph(4)-Ia\", \n",
    "         \"qnrS\",\n",
    "         \"dfrA7\",\"dfrA1\", \"drfA12\", \"dfrA14\" , \"dfrA5\", \n",
    "         \"sul2\",\"sul1\", \"sul3\" ]\n",
    "xs = range(len(f_keys))\n",
    "f_0vals = [pat_dict[k]['0'] for k in f_keys]\n",
    "f_1vals = [pat_dict[k]['1'] for k in f_keys]\n",
    "f_2vals = [pat_dict[k]['2'] for k in f_keys]\n",
    "f_3vals = [pat_dict[k]['3'] for k in f_keys]\n",
    "f_4vals = [pat_dict[k]['4'] for k in f_keys]\n",
    "f_5vals = [pat_dict[k]['5'] for k in f_keys]\n",
    "def convert_numbers(l):\n",
    "    out_list = []\n",
    "    for k in l:\n",
    "        if k != -1:\n",
    "            out_list.append(str(int(100*k)) + \"%\")\n",
    "        else:\n",
    "            out_list.append(\"N/A\")\n",
    "    return out_list\n",
    "\n",
    "f_numbers = convert_numbers([pg_dict[k][2] for k in f_keys])\n",
    "print(f_numbers)\n",
    "width = 0.5\n",
    "\n",
    "ax1.bar(xs, f_0vals, width)\n",
    "ax1.bar(xs, f_1vals, width,\n",
    "             bottom=f_0vals, label=\"ABRicate discrepant\")\n",
    "ax1.bar(xs, f_2vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i] for i in range(len(f_0vals))], label=\"ARIBA discrepant\")\n",
    "ax1.bar(xs, f_3vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i]+f_2vals[i] for i in range(len(f_0vals))], label=\"KmerResistance discrepant\")\n",
    "ax1.bar(xs, f_4vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i]+f_2vals[i]+f_3vals[i] for i in range(len(f_0vals))], label=\"SRST2 discrepant\")\n",
    "ax1.bar(xs, f_5vals, width,\n",
    "             bottom= [f_0vals[i]+ f_1vals[i]+f_2vals[i]+f_3vals[i]+f_4vals[i] for i in range(len(f_0vals))], label=\"Multiple discrepant\")\n",
    "\n",
    "ax1.set_xticks(range(len(f_keys)))\n",
    "ax1.set_xticklabels(f_keys, rotation=90)\n",
    "ax1.set_yticklabels([], rotation=90)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
