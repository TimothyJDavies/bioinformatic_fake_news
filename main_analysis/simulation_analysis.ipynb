{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial analysis script - bioinformatic differences\n",
    "\n",
    "This script goes through the first analysis step of identifying discrepancies between the 4 methods\n",
    "\n",
    "1. ABRicate (our baseline method)\n",
    "2. ARIBA\n",
    "3. KmerResistance\n",
    "4. SRST2\n",
    "\n",
    "It essentially reads in all the results we obtained, and then matches what genes were found\n",
    "Following this, it defines gene clusters, and partitions set of groups into its gene \"culster\"\n",
    "\n",
    "**I.e. The bit in the red box from the image below**\n",
    "\n",
    "![image](method_1.png)\n",
    "\n",
    "\n",
    "It then uses this output to produce **Figure X** from the paper which describes initial discrepancy for each gene family/cluster as well as identify relavent contigs for simulations\n",
    "\n",
    "### Outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "**So first steps are to load in required modules and then identify all the output reports**\n",
    "\n",
    "\n",
    "#### Dependencies\n",
    "\n",
    "1. Python 3 \n",
    "2. Biopython\n",
    "3. Pandas\n",
    "4. Numpy\n",
    "5. tqdm\n",
    "6. networkx\n",
    "\n",
    "#### Inputs\n",
    "Some notes for this step, firstly which files we take\n",
    "1. for ABRicate we take each contigs.tab file\n",
    "2. for ARIBA we use its summary file\n",
    "3. for KmerResistance we use the .KmerRes files\n",
    "4. for SRST2 we use the .out__fullgenes__seqs_clustered__results.txt files\n",
    "\n",
    "Note these were chosen as they seem to follow guidelines and where no guidelines available, give us the most closely matching results between the four programs\n",
    "\n",
    "Note also SRST2 does not produce output if it finds no genes. The others all do\n",
    "\n",
    "#### Resfinder database\n",
    "\n",
    "For this we load\n",
    "1. The naming link database\n",
    "2. For each of the sub databases by antibiotic class \n",
    "  * beta-lactam\n",
    "  * quinolone\n",
    "  * aminoglycoside\n",
    "  * sulfa antibiotics\n",
    "  * trimethoprim\n",
    "3. The whole database\n",
    "\n",
    "We do 1 so the results are interpretable\n",
    "We do 2 so we can breakdown results by antibiotic class as well as specific genes\n",
    "We do 3 to do all versus all of the resistance database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing required modules\n",
    "# File structure\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# These are the fundamental modules used for analysing the data\n",
    "\n",
    "\n",
    "# Sequence manipulation\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "# These modules are priarily used for loading the database, and then for identifying whether there are any\n",
    "# Perfrect protein matches in the dataset\n",
    "\n",
    "\n",
    "# Plotting\n",
    "\n",
    "\n",
    "# Other general\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from collections import Counter\n",
    "# We just import this to check our code runs sensibly and to get timing estimates for stuff\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the output files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Note because of the different way in which ARIBA is loaded in we also add a special reading function\n",
    "def ariba_parser(s):\n",
    "    s_clusters = sorted(list(set([k.split(\".\")[0] for k in s.index if \"cluster\" in k and \".match\" in k])))\n",
    "    s_clusters = [k for k in s_clusters if s[k+\".match\"] == \"yes\"]\n",
    "    s_genes = [s[k+ \".ref_seq\"] for k in s_clusters]\n",
    "    return s_genes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../resistance_databases/resfinder_20191001_link.csv does not exist: '../resistance_databases/resfinder_20191001_link.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a3093876e9c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 1. loading the link file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresfinder_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mresfinder_rlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinkfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/timdavies/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../resistance_databases/resfinder_20191001_link.csv does not exist: '../resistance_databases/resfinder_20191001_link.csv'"
     ]
    }
   ],
   "source": [
    "# Finally loading in the resfinder database\n",
    "linkfile = \"../resistance_databases/resfinder_20191001_link.csv\"\n",
    "resfinder_fulldb = \"../resistance_databases/resfinder_20191001_formatted.fasta\"\n",
    "blm_db = \"../resistance_databases/resfinder_20191001/beta-lactam.fsa\"\n",
    "qui_db = \"../resistance_databases/resfinder_20191001/quinolone.fsa\"\n",
    "ami_db = \"../resistance_databases/resfinder_20191001/aminoglycoside.fsa\"\n",
    "sul_db = \"../resistance_databases/resfinder_20191001/sulphonamide.fsa\"\n",
    "tri_db = \"../resistance_databases/resfinder_20191001/trimethoprim.fsa\"\n",
    "\n",
    "# 1. loading the link file\n",
    "resfinder_link = pd.read_csv(linkfile, index_col=0, header=None)\n",
    "resfinder_rlink = pd.read_csv(linkfile, index_col=1, header=None)\n",
    "\n",
    "# 2. loading the sub files\n",
    "# Note this removes duplicate Ids, this was checked as to what it did in the database formatting files\n",
    "blm_db = {k.id:k for k in SeqIO.parse(blm_db, \"fasta\")}\n",
    "qui_db = {k.id:k for k in SeqIO.parse(qui_db, \"fasta\")}\n",
    "ami_db = {k.id:k for k in SeqIO.parse(ami_db, \"fasta\")}\n",
    "sul_db = {k.id:k for k in SeqIO.parse(sul_db, \"fasta\")}\n",
    "tri_db = {k.id:k for k in SeqIO.parse(tri_db, \"fasta\")}\n",
    "\n",
    "# 3. loading the whole database\n",
    "res_db = SeqIO.to_dict(SeqIO.parse(resfinder_fulldb, \"fasta\"))\n",
    "\n",
    "# Next were going to do all vs all similarity of the resfinder database\n",
    "\n",
    "sim_matrix = pd.DataFrame(np.zeros((len(res_db.keys()),len(res_db.keys()) )), \n",
    "                         columns = sorted(list(res_db.keys())), index=sorted(list(res_db.keys())))\n",
    "# For this similarity we will use 17-mers (one of the prefixes using MASH, note several others were checked prior to this for their effects)\n",
    "# Clusters marked by this are fairly similar to other k-mer sizes\n",
    "res_db_kmers = {k: set([str(res_db[k].seq)[i:i+17] for i in range(len(res_db[k].seq)-16)]) for k in res_db.keys()}\n",
    "\n",
    "def calculate_jac_sim(l1, l2):\n",
    "    intersection = len(res_db_kmers[l1].intersection(res_db_kmers[l2]))\n",
    "    union = len(res_db_kmers[l1].union(res_db_kmers[l2]))\n",
    "    return(intersection/union)\n",
    "\n",
    "# This code actually goes through populating this matrix, \n",
    "# However takes 30 minutes to run, and for the sake of running this code quickly on line, I will\n",
    "# use a matrix I made earlier (using thie code)\n",
    "# for n in tnrange(len(res_db_kmers)):\n",
    "#     k  = list(res_db_kmers.keys())[n]\n",
    "#     for j in res_db_kmers:\n",
    "#         sim_matrix.loc[k][j] = calculate_jac_sim(k, j)\n",
    "\n",
    "sim_matrix = pd.read_csv(\"readymade_sim_matrix.csv\", index_col = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the analysis class\n",
    "\n",
    "\n",
    "In the next section of code, the aim is to define a class which performs most of the comparisons for us. \n",
    "\n",
    "#### Defining useful functions\n",
    "Before we set up the class we define useful functions\n",
    "**CLUSTERING**\n",
    "recursive cluster => This essentially uses netrowkx to greate a graph, which links togehter elements with non-zero similarity. \n",
    "The other two functions make_tuples and name list are simplications of bits within the recursive cluster functions\n",
    "\n",
    "**AGREEMENT**\n",
    "\n",
    "![image](method_combinations.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Reading in the data\n",
    "\n",
    "This does the following steps\n",
    "For each of ABRicate, ARIBA, KmerResistance, SRST2 we\n",
    "1. read in its file\n",
    "2. Pull out the TRGs it identifies and relabel them with their original names\n",
    "3. Separate these into groups according to their relavent antibiotics.\n",
    "\n",
    "\n",
    "\n",
    "#### Comparing outputs\n",
    "\n",
    "The next stage is to define what genes we have in the sample, here we then first\n",
    "1. make a combined dict of all genes found. \n",
    "2. Then separate according to clusters\n",
    "3. make identification patterns for each of these clusters\n",
    "\n",
    "\n",
    "\n",
    "This is done using an external spreadsheet (which suggests putative families for all patterns of genes seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define three useful functions we use later \n",
    "\n",
    "\n",
    "###### CLUSTERING FUNCTIONS  ######\n",
    "\n",
    "\n",
    "def make_tuples(l):\n",
    "    # Make all possible tuples in a list\n",
    "    output = []\n",
    "    for i in range(len(l)):\n",
    "        for j in range(len(l)):\n",
    "            output.append((l[i],l[j]))\n",
    "    output = sorted(list(set(output)))\n",
    "    return output\n",
    "\n",
    "def name_list(l, d):\n",
    "    #rename all elements of a list\n",
    "    return [d[k] for k in l]\n",
    "\n",
    "\n",
    "def recursive_cluster(df, l):\n",
    "    groups = {}\n",
    "    # First we get all linked pais.\n",
    "    for i in l:\n",
    "        i_data = df.loc[i]\n",
    "        i_group = [i]\n",
    "        for j in l:\n",
    "            if j != i: \n",
    "                if i_data[j] != 0:\n",
    "                    i_group.append(j)\n",
    "        i_group = sorted(i_group)\n",
    "        groups[i] =  i_group\n",
    "    # Assign numbers to the elements of l and then generate a dictionary to link numbers and names\n",
    "    naming = {}\n",
    "    reverse_naming  = {}\n",
    "    m = 1\n",
    "    for i in l:\n",
    "        naming[i] = m\n",
    "        reverse_naming[m] = i\n",
    "        m += 1\n",
    "    # Grouping tuples like a graph using networkx\n",
    "    final_tuples = []\n",
    "    for i in groups:\n",
    "        final_tuples = final_tuples + make_tuples([naming[j] for j in groups[i]])\n",
    "    final_tuples = sorted(list(set(final_tuples)))\n",
    "    graph=nx.Graph(final_tuples)\n",
    "    output = [name_list(list(c), reverse_naming) for c in nx.connected_components(graph)]\n",
    "    return output\n",
    "\n",
    "###### AGREEMENT PATTERN FUNCTIONS \n",
    "\n",
    "# Note for these functions they always assume the results are put in the correct order\n",
    "# i.e. ABRicate, ARIBA, KmerResistance, SRST2\n",
    "\n",
    "# First we start with a general agreement function \n",
    "def agreement_pattern(l1, l2, l3, l4):\n",
    "    args = deepcopy(locals())\n",
    "    arg_list = ['l1', 'l2', 'l3', 'l4']\n",
    "    for key in arg_list:\n",
    "        args[key] = \":\".join(sorted(args[key]))\n",
    "    patterns = {}\n",
    "    output = []\n",
    "    starting_no = 0\n",
    "    for key in arg_list:\n",
    "        if args[key] not in patterns:\n",
    "            starting_no += 1\n",
    "            patterns[args[key]] = starting_no\n",
    "            output.append(starting_no)\n",
    "        else:\n",
    "            output.append(patterns[args[key]])\n",
    "    return output\n",
    "    \n",
    "\n",
    "# Now for gene agreement, I use this program to say which genes (from a list) each method has found\n",
    "def pres_bin(l1, l2):\n",
    "    output = []\n",
    "    for k in l1:\n",
    "        if k in l2:\n",
    "            output.append(\"1\")\n",
    "        else:\n",
    "            output.append(\"0\")\n",
    "    return output\n",
    "\n",
    "# Here is the agreement function again, but this time i've dropped the sort function. \n",
    "# This enables me to use the output from pres_bin directly to make the patterns as defined above \n",
    "def pres_bin_agreement_pattern(l1, l2, l3, l4):\n",
    "    args = deepcopy(locals())\n",
    "    arg_list = ['l1', 'l2', 'l3', 'l4']\n",
    "    for key in arg_list:\n",
    "        args[key] = \":\".join(args[key])\n",
    "    patterns = {}\n",
    "    output = []\n",
    "    starting_no = 0\n",
    "    for key in arg_list:\n",
    "        if args[key] not in patterns:\n",
    "            starting_no += 1\n",
    "            patterns[args[key]] = starting_no\n",
    "            output.append(starting_no)\n",
    "        else:\n",
    "            output.append(patterns[args[key]])\n",
    "    return output\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading in each of the data\n",
    "\n",
    "class isolate:\n",
    "    \n",
    "    def __init__(self,guuid):\n",
    "        self.guuid = guuid\n",
    "        # First lest start with ABRicate, for this one we take everything assuming control\n",
    "        # Each section of this code does similar things, 1. read the file , 2 translate the genes, \n",
    "        # 3 separate by the classes were interested in.\n",
    "        self.abricate_fl = pd.read_csv(abricate_files[self.guuid], delimiter= \"\\t\").fillna(\"\")\n",
    "        self.abricate_fl = self.abricate_fl.loc[self.abricate_fl['%COVERAGE'] > 60.0]\n",
    "        self.abricate_fl = self.abricate_fl.loc[self.abricate_fl['%IDENTITY'] > 90.0]\n",
    "        self.abricate_genes = sorted(list(set([resfinder_link.loc[k][1] for k in list(self.abricate_fl['GENE'])])))\n",
    "        self.abricate_trg = {\"blm\":sorted([k for k in self.abricate_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.abricate_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.abricate_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.abricate_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.abricate_genes if k in tri_db.keys()])}\n",
    "        self.abricate_alltrg = sorted(list(set(self.abricate_trg['blm'] + self.abricate_trg['qui'] + \n",
    "                                               self.abricate_trg['ami'] + self.abricate_trg['sul'] +\n",
    "                                              self.abricate_trg['tri'])))\n",
    "        \n",
    "        # Next ARIBA\n",
    "        self.ariba_data = ariba_parser(ariba_summary.loc[self.guuid])\n",
    "        self.ariba_genes = sorted(list(set([resfinder_link.loc[k][1] for k in self.ariba_data])))\n",
    "        self.ariba_trg = {\"blm\":sorted([k for k in self.ariba_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.ariba_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.ariba_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.ariba_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.ariba_genes if k in tri_db.keys()])}\n",
    "        self.ariba_alltrg = sorted(list(set(self.ariba_trg['blm'] + self.ariba_trg['qui'] + \n",
    "                                           self.ariba_trg['ami'] + self.ariba_trg['sul'] + \n",
    "                                            self.ariba_trg['tri'])))\n",
    "        \n",
    "        # Next KmerResistance\n",
    "        # For this one we also add in a coverage cut-off given our file doesn't seem to be able to do this reliably\n",
    "        # Plus we are trying to apply the 70% cutoff as it doesn't work easily in the coverage\n",
    "        # So we will re-apply this.)\n",
    "        self.kmerres_fl = pd.read_csv(kmerres_files[self.guuid], delimiter = \"\\t\").fillna(\"\")\n",
    "        self.kmerres_fl = self.kmerres_fl.loc[self.kmerres_fl.template_id > 70.0]\n",
    "        self.kmerres_genes = sorted(list(set([resfinder_link.loc[k][1] for k in [j for j in list(self.kmerres_fl['#Template']) if \"resfindernewid\" in j]])))\n",
    "        self.kmerres_trg = {\"blm\":sorted([k for k in self.kmerres_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.kmerres_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.kmerres_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.kmerres_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.kmerres_genes if k in tri_db.keys()])}\n",
    "        self.kmerres_alltrg = sorted(list(set(self.kmerres_trg['blm'] + self.kmerres_trg['qui'] + self.kmerres_trg['ami'] + self.kmerres_trg['sul'] + self.kmerres_trg['tri'] )))\n",
    "        \n",
    "        # Note for SRST2 we have another bit which doesn't quite work\n",
    "        # It does not make a file if it finds no genes\n",
    "        # Therefore we put it into a try except group\n",
    "\n",
    "        try:\n",
    "            self.srst2_fl = pd.read_csv(srst2_files[self.guuid], delimiter = \"\\t\").fillna(\"\")\n",
    "            self.srst2_genes = sorted(list(set([resfinder_link.loc[k][1] for k in list(self.srst2_fl['allele'])])))\n",
    "        except:\n",
    "            self.srst2_fl = \"N/A\"\n",
    "            self.srst2_genes = []\n",
    "        self.srst2_trg = {\"blm\":sorted([k for k in self.srst2_genes if k in blm_db.keys()]),\n",
    "                             \"qui\":sorted([k for k in self.srst2_genes if k in qui_db.keys()]), \n",
    "                             \"ami\":sorted([k for k in self.srst2_genes if k in ami_db.keys()]),\n",
    "                             \"sul\":sorted([k for k in self.srst2_genes if k in sul_db.keys()]), \n",
    "                             \"tri\":sorted([k for k in self.srst2_genes if k in tri_db.keys()])} \n",
    "        self.srst2_alltrg = sorted(list(set(self.srst2_trg['blm'] + self.srst2_trg['qui'] + \n",
    "                                           self.srst2_trg['ami'] + self.srst2_trg['sul'] + \n",
    "                                            self.srst2_trg['tri'])))\n",
    "        \n",
    "        \n",
    "        ### Aggregating genes. \n",
    "        \n",
    "        self.all_trg = {}\n",
    "        for k in [\"blm\", 'qui', 'ami', 'sul', 'tri']:\n",
    "            self.all_trg[k] = sorted(list(set(self.abricate_trg[k]).union(self.ariba_trg[k]).union(self.kmerres_trg[k]).union(self.srst2_trg[k])))\n",
    "        self.geno_full = {\"abricate\":self.abricate_alltrg, \"ariba\":self.ariba_alltrg, \n",
    "                         \"kmerres\": self.kmerres_alltrg, \"srst2\":self.srst2_alltrg}\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Defining gene families \n",
    "        self.gene_families = {}\n",
    "        for k in self.all_trg.keys():\n",
    "            k_df = pd.DataFrame(np.zeros((len(self.all_trg[k]),len(self.all_trg[k]) )), \n",
    "                               columns = self.all_trg[k], index=self.all_trg[k])\n",
    "            for l in k_df.index:\n",
    "                for j in k_df.columns:\n",
    "                    k_df.loc[l][j] = sim_matrix.loc[resfinder_rlink.loc[l][0]][resfinder_rlink.loc[j][0]]\n",
    "            self.gene_families[k] = recursive_cluster(k_df, k_df.index)\n",
    "            \n",
    "            \n",
    "        ### Assessing levels of agreement\n",
    "        # Here we define three things, Firstly, do results agree for all genes for a particular antibiotic class\n",
    "        # Then do they agree for a whole isolate\n",
    "        # Then finally we do a bit more delving into the patterns of disagreement\n",
    "        # Whole isolate level agreement\n",
    "        \n",
    "        # Firstly for eac abx class\n",
    "        self.abx_patterns = {}\n",
    "        for key in [\"blm\", 'qui', 'ami', 'sul', 'tri']:\n",
    "            self.abx_patterns[key] = agreement_pattern(self.abricate_trg[key], \n",
    "                                                      self.ariba_trg[key], \n",
    "                                                      self.kmerres_trg[key], \n",
    "                                                      self.srst2_trg[key])\n",
    "        self.abx_agreement = {}\n",
    "        for key in [\"blm\", 'qui', 'ami', 'sul', 'tri']:\n",
    "            self.abx_agreement[key] = (self.abx_patterns[key] == [1, 1, 1, 1])\n",
    "\n",
    "        \n",
    "        # Then for each whole isolate\n",
    "        self.full_agreement = (list(set(list(self.abx_agreement.values()))) == [True])\n",
    "        self.full_patterns = agreement_pattern(self.abricate_alltrg, self.ariba_alltrg, \n",
    "                                               self.kmerres_alltrg, self.srst2_alltrg)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Note there is a difference between full =  all the patterns in the 5 gene families I have\n",
    "        # and full across the entiritty of resfinder , this attribute reflects this\n",
    "        self.wholeresfinder_patterns = agreement_pattern(sorted(self.abricate_genes), sorted(self.ariba_genes),\n",
    "                                               sorted(self.kmerres_genes), sorted(self.srst2_genes))\n",
    "        self.wholeresfinder_agreement = (self.wholeresfinder_patterns == [1,1,1,1])\n",
    "\n",
    "\n",
    "        \n",
    "        # Lastly for each gene\n",
    "        self.genes_identified = {}\n",
    "        self.gene_patterns = {}\n",
    "        self.gene_group = {}\n",
    "        for abx in self.gene_families:\n",
    "            for pat in self.gene_families[abx]:\n",
    "                pat_id = \":\".join(pat)\n",
    "                pat_string = [pres_bin(pat, self.abricate_alltrg),\n",
    "                              pres_bin(pat, self.ariba_alltrg), \n",
    "                              pres_bin(pat, self.kmerres_alltrg), \n",
    "                              pres_bin(pat, self.srst2_alltrg)]\n",
    "                self.gene_patterns[pat_id] = pres_bin_agreement_pattern(pat_string[0], pat_string[1], pat_string[2], pat_string[3])\n",
    "                pat_string = \"|\".join([\":\".join(i) for i in pat_string])\n",
    "                self.genes_identified[pat_id] = pat_string\n",
    "                self.gene_group[pat_id] = abx\n",
    "\n",
    "\n",
    "# Now with the classes set up we read in everything into an isolates dict\n",
    "isolates = {}\n",
    "\n",
    "\n",
    "for run in [str(i) for i in range(1,11)]:\n",
    "    abricate_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/simulation_run{0}/collated_output/abricate_summary/\" .format(run)\n",
    "    ariba_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/simulation_run{0}/collated_output/ariba_summary/\" .format(run)\n",
    "    kmerres_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/simulation_run{0}/collated_output/kmerres_summary/\" .format(run)\n",
    "    srst2_dir = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/simulation_run{0}/collated_output/srst2_summary/\" .format(run)\n",
    "\n",
    "    ariba_summary = \"/Users/timdavies/Dropbox_Temp_Outside/pl_comp_data/simulation_run{0}/collated_output/ariba_summary.csv\" .format(run)\n",
    "\n",
    "    # ABRicate\n",
    "    abricate_files = [os.path.join(root, f) for root, dirs, files \n",
    "                      in os.walk(abricate_dir)\n",
    "                     for f in files if f != \"summary.tab\"]\n",
    "    guuids = [k.split(\"/\")[-1].rstrip(\"_assem.tab\") for k in abricate_files]\n",
    "    abricate_files = {k.split(\"/\")[-1].rstrip(\"_assem.tab\"):k for k in abricate_files}\n",
    "\n",
    "    #KmerResistance\n",
    "    kmerres_files = [os.path.join(root, f) for root, dirs, files in os.walk(kmerres_dir)\n",
    "                     for f in files if \".KmerRes\" in f]\n",
    "    kmerres_files = {k.split(\"/\")[-1].rstrip(\".KmerRes\"):k for k in kmerres_files}\n",
    "\n",
    "    #SRST2\n",
    "    srst2_files = [os.path.join(root, f) for root, dirs, files in os.walk(srst2_dir) for f in files]\n",
    "    srst2_files = {k.split(\"/\")[-1].split(\"_SRST\")[0]:k for k in srst2_files}\n",
    "\n",
    "    #ARIBA\n",
    "    ariba_names = [k.split(\"/\")[1].split(\".\")[0] for k in list(pd.read_csv(ariba_summary).name)]\n",
    "    ariba_summary = pd.read_csv(ariba_summary).fillna(\"\")\n",
    "    ariba_summary.index = ariba_names\n",
    "    isolates[\"run_{0}\" .format(run)] = {}\n",
    "    for n in tnrange(len(guuids)):\n",
    "        k = guuids[n]\n",
    "        x = isolate(k)\n",
    "        isolates[\"run_{0}\" .format(run)][k] = x\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step lets have a look at the original mutations\n",
    "simulations_contigs = pd.read_csv(\"simulation_contigs.csv\")\n",
    "print(len(simulations_contigs))\n",
    "\n",
    "true_true = 0\n",
    "true_false = 0\n",
    "false_false = 0\n",
    "tt_pats = []\n",
    "tf_pats =[]\n",
    "ff_pats = []\n",
    "\n",
    "with open(\"interpreting_simulations.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter = \",\")\n",
    "    writer.writerow([\"isolate\", \"pattern\", \"simulatable\", \"run_1\", \"run_2\", \"run_3\", \"run_4\", \"run_5\", \"run_6\", \n",
    "                     \"run_7\", \"run_8\", \n",
    "                     \"run_9\", \"run_10\", \n",
    "                     \"overall\"])\n",
    "    for k in range(len(simulations_contigs)):\n",
    "        k_data = simulations_contigs.iloc[k]\n",
    "        k_pattern = k_data.pattern\n",
    "        k_isolate = k_data.isolate\n",
    "        k_simulatable = k_data.simulatable\n",
    "        out_row = [k_isolate, k_pattern,k_simulatable]\n",
    "        for run in range(1, 11):\n",
    "            try:\n",
    "                k_contig = k_data.contig.split(\"_length_\")[0]\n",
    "                k_key = k_isolate + \"_\" + k_contig\n",
    "                if k_pattern in isolates[\"run_{0}\" .format(run)][k_key].genes_identified.keys():\n",
    "                    true_true += 1\n",
    "                    out_row.append(\"True\")\n",
    "                    tt_pats.append(k_pattern)\n",
    "                else:\n",
    "                    true_false += 1\n",
    "                    out_row.append(\"False\")\n",
    "                    tf_pats.append(k_pattern)\n",
    "            except:\n",
    "                false_false += 1\n",
    "                out_row.append(\"False\")\n",
    "                ff_pats.append(k_pattern)\n",
    "        if \"True\" in out_row[-2:]:\n",
    "            out_row.append(\"True\")\n",
    "        else:\n",
    "            out_row.append(\"False\")\n",
    "        writer.writerow(out_row)\n",
    "\n",
    "print(true_true, true_false, false_false)\n",
    "\n",
    "# for k in Counter(tt_pats):\n",
    "#     print(Counter(tt_pats)[k], k)\n",
    "# print('')\n",
    "# print(\"\")\n",
    "for k in sorted(Counter(tf_pats).keys(), key = lambda a: Counter(tf_pats)[a], reverse = True):\n",
    "    print(Counter(tf_pats)[k], k, k in tt_pats)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(isolates['59e792c9-73db-426e-9ee0-fc63ccc700af_NODE_61'].genes_identified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
